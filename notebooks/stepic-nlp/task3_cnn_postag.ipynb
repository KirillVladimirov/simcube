{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pyconll\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import simcube\n",
    "from simcube.data import \\\n",
    "    tokenize_corpus, \\\n",
    "    build_vocabulary, \\\n",
    "    character_tokenize, \\\n",
    "    pos_corpus_to_tensor, \\\n",
    "    POSTagger\n",
    "from simcube.pipeline import \\\n",
    "    train_eval_loop, \\\n",
    "    predict_with_model, \\\n",
    "    init_random_seed\n",
    "\n",
    "init_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка текстов и разбиение на обучающую и тестовую подвыборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('../../data/raw/rus_tag_corpus/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('../../data/raw/rus_tag_corpus/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета NOUN\n",
      ". PUNCT\n",
      "\n",
      "\n",
      "Начальник NOUN\n",
      "областного ADJ\n",
      "управления NOUN\n",
      "связи NOUN\n",
      "Семен PROPN\n",
      "Еремеевич PROPN\n",
      "был AUX\n",
      "человек NOUN\n",
      "простой ADJ\n",
      ", PUNCT\n",
      "приходил VERB\n",
      "на ADP\n",
      "работу NOUN\n",
      "всегда ADV\n",
      "вовремя ADV\n",
      ", PUNCT\n",
      "здоровался VERB\n",
      "с ADP\n",
      "секретаршей NOUN\n",
      "за ADP\n",
      "руку NOUN\n",
      "и CCONJ\n",
      "иногда ADV\n",
      "даже PART\n",
      "писал VERB\n",
      "в ADP\n",
      "стенгазету NOUN\n",
      "заметки NOUN\n",
      "под ADP\n",
      "псевдонимом NOUN\n",
      "\" PUNCT\n",
      "Муха NOUN\n",
      "\" PUNCT\n",
      ". PUNCT\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in full_train[:2]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наибольшая длина предложения 205\n",
      "Наибольшая длина токена 47\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
    "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета .\n",
      "Начальник областного управления связи Семен Еремеевич был человек простой , приходил на работу всегда вовремя , здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом \" Муха \" .\n",
      "В приемной его с утра ожидали посетители , - кое-кто с важными делами , а кое-кто и с такими , которые легко можно было решить в нижестоящих инстанциях , не затрудняя Семена Еремеевича .\n",
      "Однако стиль работы Семена Еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .\n",
      "Приемная была обставлена просто , но по-деловому .\n",
      "У двери стоял стол секретарши , на столе - пишущая машинка с широкой кареткой .\n",
      "В углу висел репродуктор и играло радио для развлечения ожидающих и еще для того , чтобы заглушать голос начальника , доносившийся из кабинета , так как , бесспорно , среди посетителей могли находиться и случайные люди .\n",
      "Кабинет отличался скромностью , присущей Семену Еремеевичу .\n",
      "В глубине стоял широкий письменный стол с бронзовыми чернильницами и перед ним два кожаных кресла .\n",
      "Справа был стол для заседаний - длинный , накрытый зеленым сукном и с обеих сторон аккуратно заставленный стульями .\n"
     ]
    }
   ],
   "source": [
    "all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\n",
    "print('\\n'.join(all_train_texts[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных символов 150\n",
      "[('<PAD>', 0), (' ', 1), ('о', 2), ('е', 3), ('а', 4), ('т', 5), ('и', 6), ('н', 7), ('.', 8), ('с', 9)]\n"
     ]
    }
   ],
   "source": [
    "train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\n",
    "char_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, max_doc_freq=1.0, min_count=5, pad_word='<PAD>')\n",
    "print(\"Количество уникальных символов\", len(char_vocab))\n",
    "print(list(char_vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " ' ': 1,\n",
       " 'о': 2,\n",
       " 'е': 3,\n",
       " 'а': 4,\n",
       " 'т': 5,\n",
       " 'и': 6,\n",
       " 'н': 7,\n",
       " '.': 8,\n",
       " 'с': 9,\n",
       " 'р': 10,\n",
       " 'л': 11,\n",
       " 'в': 12,\n",
       " 'к': 13,\n",
       " 'д': 14,\n",
       " 'м': 15,\n",
       " 'п': 16,\n",
       " 'у': 17,\n",
       " 'ы': 18,\n",
       " 'я': 19,\n",
       " 'ь': 20,\n",
       " 'з': 21,\n",
       " 'г': 22,\n",
       " 'б': 23,\n",
       " ',': 24,\n",
       " 'ч': 25,\n",
       " 'й': 26,\n",
       " 'ж': 27,\n",
       " 'х': 28,\n",
       " 'ш': 29,\n",
       " 'ю': 30,\n",
       " 'ц': 31,\n",
       " '-': 32,\n",
       " 'щ': 33,\n",
       " 'э': 34,\n",
       " 'ф': 35,\n",
       " 'В': 36,\n",
       " 'П': 37,\n",
       " '\"': 38,\n",
       " 'Н': 39,\n",
       " 'С': 40,\n",
       " 'К': 41,\n",
       " 'О': 42,\n",
       " 'И': 43,\n",
       " 'М': 44,\n",
       " 'А': 45,\n",
       " 'Р': 46,\n",
       " 'Т': 47,\n",
       " ':': 48,\n",
       " 'Д': 49,\n",
       " '0': 50,\n",
       " 'Г': 51,\n",
       " '1': 52,\n",
       " 'Б': 53,\n",
       " 'Е': 54,\n",
       " '?': 55,\n",
       " ')': 56,\n",
       " '(': 57,\n",
       " '2': 58,\n",
       " 'Э': 59,\n",
       " 'У': 60,\n",
       " 'З': 61,\n",
       " 'Л': 62,\n",
       " '5': 63,\n",
       " 'ъ': 64,\n",
       " 'Ч': 65,\n",
       " 'Ф': 66,\n",
       " '9': 67,\n",
       " '3': 68,\n",
       " '!': 69,\n",
       " 'Я': 70,\n",
       " 'Ш': 71,\n",
       " '4': 72,\n",
       " '8': 73,\n",
       " 'Х': 74,\n",
       " '6': 75,\n",
       " '_': 76,\n",
       " '7': 77,\n",
       " 'Ц': 78,\n",
       " '…': 79,\n",
       " 'e': 80,\n",
       " '%': 81,\n",
       " 'a': 82,\n",
       " 'r': 83,\n",
       " 'o': 84,\n",
       " 'i': 85,\n",
       " 'I': 86,\n",
       " 'Ж': 87,\n",
       " 'n': 88,\n",
       " 't': 89,\n",
       " 's': 90,\n",
       " 'Ю': 91,\n",
       " 'l': 92,\n",
       " ';': 93,\n",
       " 'u': 94,\n",
       " 'X': 95,\n",
       " 'ё': 96,\n",
       " 'c': 97,\n",
       " 'm': 98,\n",
       " 'M': 99,\n",
       " 'd': 100,\n",
       " 'B': 101,\n",
       " 'C': 102,\n",
       " 'S': 103,\n",
       " 'A': 104,\n",
       " 'p': 105,\n",
       " 'P': 106,\n",
       " 'h': 107,\n",
       " 'V': 108,\n",
       " 'R': 109,\n",
       " 'g': 110,\n",
       " 'y': 111,\n",
       " 'T': 112,\n",
       " 'E': 113,\n",
       " 'b': 114,\n",
       " '$': 115,\n",
       " 'Й': 116,\n",
       " 'k': 117,\n",
       " 'D': 118,\n",
       " 'F': 119,\n",
       " 'N': 120,\n",
       " 'Ы': 121,\n",
       " 'L': 122,\n",
       " 'G': 123,\n",
       " 'Ь': 124,\n",
       " 'w': 125,\n",
       " 'H': 126,\n",
       " '/': 127,\n",
       " 'v': 128,\n",
       " 'x': 129,\n",
       " 'K': 130,\n",
       " 'f': 131,\n",
       " 'W': 132,\n",
       " 'O': 133,\n",
       " 'Щ': 134,\n",
       " '№': 135,\n",
       " 'U': 136,\n",
       " 'z': 137,\n",
       " 'J': 138,\n",
       " \"'\": 139,\n",
       " 'Y': 140,\n",
       " 'j': 141,\n",
       " 'Z': 142,\n",
       " '+': 143,\n",
       " '^': 144,\n",
       " 'Ъ': 145,\n",
       " '°': 146,\n",
       " 'Q': 147,\n",
       " '€': 148,\n",
       " '=': 149}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<NOTAG>': 0,\n",
       " 'ADJ': 1,\n",
       " 'ADP': 2,\n",
       " 'ADV': 3,\n",
       " 'AUX': 4,\n",
       " 'CCONJ': 5,\n",
       " 'DET': 6,\n",
       " 'INTJ': 7,\n",
       " 'NOUN': 8,\n",
       " 'NUM': 9,\n",
       " 'PART': 10,\n",
       " 'PRON': 11,\n",
       " 'PROPN': 12,\n",
       " 'PUNCT': 13,\n",
       " 'SCONJ': 14,\n",
       " 'SYM': 15,\n",
       " 'VERB': 16,\n",
       " 'X': 17}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\n",
    "label2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "\n",
    "test_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 39,  4, 25,  4, 11, 20,  7,  6, 13,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  2, 23, 11,  4,  9,  5,  7,  2, 22,  2,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0, 17, 16, 10,  4, 12, 11,  3,  7,  6, 19,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  9, 12, 19, 21,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0, 40,  3, 15,  3,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8,  1,  8,  8, 12, 12,  4,  8,  1, 13, 16,  2,  8,  3,  3, 13, 16,  2,\n",
       "         8,  2,  8,  5,  3, 10, 16,  2,  8,  8,  2,  8, 13,  8, 13, 13,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вспомогательная свёрточная архитектура"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedConv1d(nn.Module):\n",
    "    def __init__(self, features_num, layers_n=1, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(layers_n):\n",
    "            layers.append(nn.Sequential(\n",
    "                conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.LeakyReLU()))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание частей речи на уровне отдельных токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эта сеть не использует контекст токенов, в предсказании части речи участвуют только символы самого токена"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTokenPOSTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, labels_num, embedding_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.backbone = StackedConv1d(embedding_size, **kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Linear(embedding_size, labels_num)\n",
    "        self.labels_num = labels_num\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
    "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
    "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n",
    "        \n",
    "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
    "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
    "        \n",
    "        features = self.backbone(char_embeddings)\n",
    "        \n",
    "        global_features = self.global_pooling(features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n",
    "        \n",
    "        logits_flat = self.out(global_features)  # BatchSize*MaxSentenceLen x LabelsNum\n",
    "        logits = logits_flat.view(batch_size, max_sent_len, self.labels_num)  # BatchSize x MaxSentenceLen x LabelsNum\n",
    "        logits = logits.permute(0, 2, 1)  # BatchSize x LabelsNum x MaxSentenceLen\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров 47826\n"
     ]
    }
   ],
   "source": [
    "single_token_model = SingleTokenPOSTagger(len(char_vocab), len(label2id), embedding_size=64, layers_n=3, kernel_size=3, dropout=0.3)\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in single_token_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбранное устройство: cuda\n",
      "Эпоха 0\n",
      "Эпоха: 501 итераций, 201.55 сек\n",
      "Среднее значение функции потерь на обучении 0.08007763111276064\n",
      "Среднее значение функции потерь на валидации 0.03311723077872602\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 1\n",
      "Эпоха: 501 итераций, 202.19 сек\n",
      "Среднее значение функции потерь на обучении 0.02833438031152337\n",
      "Среднее значение функции потерь на валидации 0.026528122825640262\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 2\n",
      "Эпоха: 501 итераций, 197.15 сек\n",
      "Среднее значение функции потерь на обучении 0.024013424901934918\n",
      "Среднее значение функции потерь на валидации 0.024688546093973782\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 3\n",
      "Эпоха: 501 итераций, 197.56 сек\n",
      "Среднее значение функции потерь на обучении 0.02207319940181966\n",
      "Среднее значение функции потерь на валидации 0.021894465412686366\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 4\n",
      "Эпоха: 501 итераций, 197.53 сек\n",
      "Среднее значение функции потерь на обучении 0.021079481579363346\n",
      "Среднее значение функции потерь на валидации 0.02251679018729984\n",
      "\n",
      "Эпоха 5\n",
      "Эпоха: 501 итераций, 197.79 сек\n",
      "Среднее значение функции потерь на обучении 0.020209305029280648\n",
      "Среднее значение функции потерь на валидации 0.021233664401391947\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 6\n",
      "Эпоха: 501 итераций, 197.76 сек\n",
      "Среднее значение функции потерь на обучении 0.01959674152361597\n",
      "Среднее значение функции потерь на валидации 0.024181888042257564\n",
      "\n",
      "Эпоха 7\n",
      "Эпоха: 501 итераций, 197.72 сек\n",
      "Среднее значение функции потерь на обучении 0.019172200719322154\n",
      "Среднее значение функции потерь на валидации 0.02069726960715091\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 8\n",
      "Эпоха: 501 итераций, 198.36 сек\n",
      "Среднее значение функции потерь на обучении 0.018765757051844915\n",
      "Среднее значение функции потерь на валидации 0.020073623433340303\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 9\n",
      "Эпоха: 501 итераций, 198.73 сек\n",
      "Среднее значение функции потерь на обучении 0.018283182874083995\n",
      "Среднее значение функции потерь на валидации 0.019145250477191834\n",
      "Новая лучшая модель!\n",
      "\n",
      "CPU times: user 33min 58s, sys: 1.53 s, total: 34min\n",
      "Wall time: 33min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_val_loss, best_single_token_model = train_eval_loop(single_token_model, train_dataset, test_dataset,\n",
    "    F.cross_entropy,\n",
    "    lr=5e-3,\n",
    "    epoch_n=10,\n",
    "    batch_size=64,\n",
    "    device='cuda',\n",
    "    early_stopping_patience=5,\n",
    "    max_batches_per_epoch_train=500,\n",
    "    max_batches_per_epoch_val=100,\n",
    "    lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2, factor=0.5, verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "torch.save(best_single_token_model.state_dict(), '../../data/models/single_token_pos.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "single_token_model.load_state_dict(torch.load('../../data/models/single_token_pos.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1525.4375 [00:00<00:41, 36.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбранное устройство: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1526it [00:40, 38.12it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на обучении 0.01657356694340706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/205.75 [00:00<00:05, 36.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   9136391\n",
      "         ADJ       0.88      0.94      0.91     85589\n",
      "         ADP       1.00      0.99      0.99     81963\n",
      "         ADV       0.87      0.91      0.89     44101\n",
      "         AUX       0.85      0.75      0.80      7522\n",
      "       CCONJ       0.87      0.99      0.93     30432\n",
      "         DET       0.85      0.81      0.83     21968\n",
      "        INTJ       0.78      0.18      0.29        78\n",
      "        NOUN       0.98      0.92      0.95    214497\n",
      "         NUM       0.97      0.93      0.95     13746\n",
      "        PART       0.99      0.76      0.86     26651\n",
      "        PRON       0.88      0.85      0.87     38438\n",
      "       PROPN       0.79      0.96      0.87     32401\n",
      "       PUNCT       1.00      1.00      1.00    157989\n",
      "       SCONJ       0.80      0.88      0.84     16219\n",
      "         SYM       1.00      0.99      1.00       840\n",
      "        VERB       0.92      0.94      0.93     97670\n",
      "           X       0.94      0.61      0.74       375\n",
      "\n",
      "    accuracy                           0.99  10006870\n",
      "   macro avg       0.91      0.86      0.87  10006870\n",
      "weighted avg       0.99      0.99      0.99  10006870\n",
      "\n",
      "\n",
      "Выбранное устройство: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/205.75 [00:05<00:00, 39.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на валидации 0.018714211881160736\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   1231232\n",
      "         ADJ       0.87      0.93      0.90     11222\n",
      "         ADP       1.00      0.99      0.99     10585\n",
      "         ADV       0.87      0.91      0.89      6165\n",
      "         AUX       0.83      0.81      0.82      1106\n",
      "       CCONJ       0.88      0.99      0.93      4410\n",
      "         DET       0.83      0.80      0.81      3085\n",
      "        INTJ       1.00      0.09      0.17        11\n",
      "        NOUN       0.98      0.92      0.95     27974\n",
      "         NUM       0.96      0.90      0.93      1829\n",
      "        PART       0.99      0.76      0.86      3877\n",
      "        PRON       0.88      0.83      0.86      5598\n",
      "       PROPN       0.79      0.95      0.86      4438\n",
      "       PUNCT       1.00      1.00      1.00     22694\n",
      "       SCONJ       0.77      0.90      0.83      2258\n",
      "         SYM       1.00      0.96      0.98        53\n",
      "        VERB       0.92      0.94      0.93     13078\n",
      "           X       0.99      0.78      0.87       105\n",
      "\n",
      "    accuracy                           0.99   1349720\n",
      "   macro avg       0.92      0.86      0.87   1349720\n",
      "weighted avg       0.99      0.99      0.99   1349720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pred = predict_with_model(single_token_model, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(single_token_model, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание частей речи на уровне предложений (с учётом контекста)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceLevelPOSTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.single_token_backbone = StackedConv1d(embedding_size, **single_backbone_kwargs)\n",
    "        self.context_backbone = StackedConv1d(embedding_size, **context_backbone_kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n",
    "        self.labels_num = labels_num\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
    "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
    "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n",
    "        \n",
    "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
    "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
    "        char_features = self.single_token_backbone(char_embeddings)\n",
    "        \n",
    "        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n",
    "\n",
    "        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n",
    "        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n",
    "        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n",
    "\n",
    "        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров 84882\n"
     ]
    }
   ],
   "source": [
    "sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
    "                                              single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3),\n",
    "                                              context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3))\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбранное устройство: cuda\n",
      "Эпоха 0\n",
      "Эпоха: 501 итераций, 203.88 сек\n",
      "Среднее значение функции потерь на обучении 0.07464262613740391\n",
      "Среднее значение функции потерь на валидации 0.026716668287863825\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 1\n",
      "Эпоха: 501 итераций, 203.05 сек\n",
      "Среднее значение функции потерь на обучении 0.024460480539146772\n",
      "Среднее значение функции потерь на валидации 0.020085257736246774\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 2\n",
      "Эпоха: 501 итераций, 204.39 сек\n",
      "Среднее значение функции потерь на обучении 0.020006924592508767\n",
      "Среднее значение функции потерь на валидации 0.016401978292603894\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 3\n",
      "Эпоха: 501 итераций, 202.78 сек\n",
      "Среднее значение функции потерь на обучении 0.017660559854003603\n",
      "Среднее значение функции потерь на валидации 0.014658478766421575\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 4\n",
      "Эпоха: 501 итераций, 202.77 сек\n",
      "Среднее значение функции потерь на обучении 0.01652661173933459\n",
      "Среднее значение функции потерь на валидации 0.014009611044853631\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 5\n",
      "Эпоха: 501 итераций, 202.75 сек\n",
      "Среднее значение функции потерь на обучении 0.015650754335323376\n",
      "Среднее значение функции потерь на валидации 0.013239729416436783\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 6\n",
      "Эпоха: 501 итераций, 202.70 сек\n",
      "Среднее значение функции потерь на обучении 0.01514789741486311\n",
      "Среднее значение функции потерь на валидации 0.012593350645228483\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 7\n",
      "Эпоха: 501 итераций, 204.34 сек\n",
      "Среднее значение функции потерь на обучении 0.01471817716509996\n",
      "Среднее значение функции потерь на валидации 0.011603421144334986\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 8\n",
      "Эпоха: 501 итераций, 202.84 сек\n",
      "Среднее значение функции потерь на обучении 0.014371817583913456\n",
      "Среднее значение функции потерь на валидации 0.012567314130281754\n",
      "\n",
      "Эпоха 9\n",
      "Эпоха: 501 итераций, 203.28 сек\n",
      "Среднее значение функции потерь на обучении 0.014193252546939546\n",
      "Среднее значение функции потерь на валидации 0.011477569602245446\n",
      "Новая лучшая модель!\n",
      "\n",
      "CPU times: user 34min 44s, sys: 812 ms, total: 34min 44s\n",
      "Wall time: 34min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_val_loss, best_sentence_level_model = train_eval_loop(sentence_level_model, train_dataset, test_dataset,\n",
    "    F.cross_entropy,\n",
    "    lr=5e-3,\n",
    "    epoch_n=10,\n",
    "    batch_size=64,\n",
    "    device='cuda',\n",
    "    early_stopping_patience=5,\n",
    "    max_batches_per_epoch_train=500,\n",
    "    max_batches_per_epoch_val=100,\n",
    "    lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2, factor=0.5, verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "torch.save(best_sentence_level_model.state_dict(), '../../data/models/sentence_level_pos.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "sentence_level_model.load_state_dict(torch.load('../../data/models/sentence_level_pos.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1525.4375 [00:00<00:44, 34.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбранное устройство: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1526it [00:40, 37.88it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на обучении 0.01012236624956131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/205.75 [00:00<00:05, 36.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   9136391\n",
      "         ADJ       0.93      0.94      0.94     85589\n",
      "         ADP       1.00      0.98      0.99     81963\n",
      "         ADV       0.91      0.92      0.92     44101\n",
      "         AUX       0.90      0.92      0.91      7522\n",
      "       CCONJ       0.94      0.98      0.96     30432\n",
      "         DET       0.92      0.92      0.92     21968\n",
      "        INTJ       0.86      0.24      0.38        78\n",
      "        NOUN       0.97      0.97      0.97    214497\n",
      "         NUM       0.96      0.95      0.95     13746\n",
      "        PART       0.97      0.88      0.92     26651\n",
      "        PRON       0.95      0.93      0.94     38438\n",
      "       PROPN       0.92      0.97      0.95     32401\n",
      "       PUNCT       1.00      1.00      1.00    157989\n",
      "       SCONJ       0.90      0.92      0.91     16219\n",
      "         SYM       1.00      1.00      1.00       840\n",
      "        VERB       0.95      0.97      0.96     97670\n",
      "           X       0.84      0.69      0.76       375\n",
      "\n",
      "    accuracy                           1.00  10006870\n",
      "   macro avg       0.94      0.90      0.91  10006870\n",
      "weighted avg       1.00      1.00      1.00  10006870\n",
      "\n",
      "\n",
      "Выбранное устройство: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/205.75 [00:05<00:00, 38.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на валидации 0.011377825401723385\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   1231232\n",
      "         ADJ       0.93      0.93      0.93     11222\n",
      "         ADP       1.00      0.99      0.99     10585\n",
      "         ADV       0.92      0.91      0.91      6165\n",
      "         AUX       0.88      0.91      0.90      1106\n",
      "       CCONJ       0.94      0.98      0.96      4410\n",
      "         DET       0.90      0.91      0.91      3085\n",
      "        INTJ       1.00      0.27      0.43        11\n",
      "        NOUN       0.97      0.96      0.97     27974\n",
      "         NUM       0.95      0.94      0.94      1829\n",
      "        PART       0.97      0.88      0.92      3877\n",
      "        PRON       0.94      0.92      0.93      5598\n",
      "       PROPN       0.91      0.97      0.94      4438\n",
      "       PUNCT       1.00      1.00      1.00     22694\n",
      "       SCONJ       0.88      0.91      0.90      2258\n",
      "         SYM       1.00      0.98      0.99        53\n",
      "        VERB       0.93      0.96      0.95     13078\n",
      "           X       0.91      0.83      0.87       105\n",
      "\n",
      "    accuracy                           1.00   1349720\n",
      "   macro avg       0.95      0.90      0.91   1349720\n",
      "weighted avg       1.00      1.00      1.00   1349720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pred = predict_with_model(sentence_level_model, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(sentence_level_model, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceLevelPOSTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.single_token_backbone = StackedConv1d(embedding_size, **single_backbone_kwargs)\n",
    "        self.bn1 = nn.BatchNorm1d(embedding_size)\n",
    "        self.context_backbone = StackedConv1d(embedding_size, **context_backbone_kwargs)\n",
    "        self.bn2 = nn.BatchNorm1d(embedding_size)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n",
    "        self.labels_num = labels_num\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
    "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
    "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n",
    "        \n",
    "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
    "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
    "        char_features = self.bn1(self.single_token_backbone(char_embeddings))\n",
    "        \n",
    "        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n",
    "\n",
    "        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n",
    "        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n",
    "        context_features = self.bn2(self.context_backbone(token_features))  # BatchSize x EmbSize x MaxSentenceLen\n",
    "\n",
    "        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров 85138\n"
     ]
    }
   ],
   "source": [
    "sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
    "                                              single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3),\n",
    "                                              context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3))\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбранное устройство: cuda\n",
      "Эпоха 0\n",
      "Эпоха: 382 итераций, 317.19 сек\n",
      "Среднее значение функции потерь на обучении 0.04765604019009006\n",
      "Среднее значение функции потерь на валидации 0.017542734563064117\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 1\n",
      "Эпоха: 382 итераций, 315.85 сек\n",
      "Среднее значение функции потерь на обучении 0.017847579177371495\n",
      "Среднее значение функции потерь на валидации 0.012863060074428527\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 2\n",
      "Эпоха: 382 итераций, 315.26 сек\n",
      "Среднее значение функции потерь на обучении 0.015498362813164426\n",
      "Среднее значение функции потерь на валидации 0.01461146050132811\n",
      "\n",
      "Эпоха 3\n",
      "Эпоха: 382 итераций, 315.53 сек\n",
      "Среднее значение функции потерь на обучении 0.014310227353515894\n",
      "Среднее значение функции потерь на валидации 0.03934819623827934\n",
      "\n",
      "Эпоха 4\n",
      "Эпоха: 382 итераций, 315.19 сек\n",
      "Среднее значение функции потерь на обучении 0.013505877546615001\n",
      "Среднее значение функции потерь на валидации 0.08965974773925084\n",
      "Epoch     5: reducing learning rate of group 0 to 2.5000e-03.\n",
      "\n",
      "Эпоха 5\n",
      "Эпоха: 382 итераций, 316.03 сек\n",
      "Среднее значение функции потерь на обучении 0.011848538325299845\n",
      "Среднее значение функции потерь на валидации 0.04605094938037487\n",
      "\n",
      "Эпоха 6\n",
      "Эпоха: 382 итераций, 315.80 сек\n",
      "Среднее значение функции потерь на обучении 0.011417155480743704\n",
      "Среднее значение функции потерь на валидации 0.02612368991741767\n",
      "\n",
      "Эпоха 7\n",
      "Эпоха: 382 итераций, 315.58 сек\n",
      "Среднее значение функции потерь на обучении 0.011182349606743814\n",
      "Среднее значение функции потерь на валидации 0.06039789992456253\n",
      "Модель не улучшилась за последние 5 эпох, прекращаем обучение\n",
      "CPU times: user 42min 50s, sys: 796 ms, total: 42min 51s\n",
      "Wall time: 42min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_val_loss, best_sentence_level_model = train_eval_loop(sentence_level_model, train_dataset, test_dataset,\n",
    "    F.cross_entropy,\n",
    "    lr=5e-3,\n",
    "    epoch_n=20,\n",
    "    batch_size=128,\n",
    "    device='cuda',\n",
    "    early_stopping_patience=5,\n",
    "    max_batches_per_epoch_train=500,\n",
    "    max_batches_per_epoch_val=100,\n",
    "    lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2, factor=0.5, verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1525.4375 [00:00<00:44, 34.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбранное устройство: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1526it [00:42, 35.99it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на обучении 0.06275895237922668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/205.75 [00:00<00:05, 34.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   9136391\n",
      "         ADJ       0.96      0.94      0.95     85589\n",
      "         ADP       1.00      1.00      1.00     81963\n",
      "         ADV       0.93      0.94      0.93     44101\n",
      "         AUX       0.86      0.97      0.91      7522\n",
      "       CCONJ       0.95      0.97      0.96     30432\n",
      "         DET       0.92      0.93      0.93     21968\n",
      "        INTJ       0.94      0.21      0.34        78\n",
      "        NOUN       0.98      0.98      0.98    214497\n",
      "         NUM       0.96      0.96      0.96     13746\n",
      "        PART       0.94      0.93      0.93     26651\n",
      "        PRON       0.96      0.93      0.95     38438\n",
      "       PROPN       0.96      0.96      0.96     32401\n",
      "       PUNCT       1.00      1.00      1.00    157989\n",
      "       SCONJ       0.88      0.98      0.92     16219\n",
      "         SYM       1.00      1.00      1.00       840\n",
      "        VERB       0.97      0.96      0.96     97670\n",
      "           X       0.95      0.66      0.78       375\n",
      "\n",
      "    accuracy                           1.00  10006870\n",
      "   macro avg       0.95      0.91      0.91  10006870\n",
      "weighted avg       1.00      1.00      1.00  10006870\n",
      "\n",
      "\n",
      "Выбранное устройство: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/205.75 [00:05<00:00, 36.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на валидации 0.06050407141447067\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   1231232\n",
      "         ADJ       0.96      0.93      0.94     11222\n",
      "         ADP       1.00      1.00      1.00     10585\n",
      "         ADV       0.92      0.93      0.92      6165\n",
      "         AUX       0.86      0.95      0.91      1106\n",
      "       CCONJ       0.95      0.97      0.96      4410\n",
      "         DET       0.91      0.92      0.91      3085\n",
      "        INTJ       0.60      0.27      0.37        11\n",
      "        NOUN       0.98      0.98      0.98     27974\n",
      "         NUM       0.95      0.95      0.95      1829\n",
      "        PART       0.93      0.93      0.93      3877\n",
      "        PRON       0.96      0.92      0.94      5598\n",
      "       PROPN       0.96      0.94      0.95      4438\n",
      "       PUNCT       1.00      1.00      1.00     22694\n",
      "       SCONJ       0.86      0.97      0.91      2258\n",
      "         SYM       1.00      1.00      1.00        53\n",
      "        VERB       0.96      0.96      0.96     13078\n",
      "           X       0.98      0.77      0.86       105\n",
      "\n",
      "    accuracy                           1.00   1349720\n",
      "   macro avg       0.93      0.91      0.92   1349720\n",
      "weighted avg       1.00      1.00      1.00   1349720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pred = predict_with_model(sentence_level_model, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(sentence_level_model, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "torch.save(best_sentence_level_model.state_dict(), '../../data/models/sentence_level_pos.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_level_pos_tagger = POSTagger(sentence_level_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    'Мама мыла раму.',\n",
    "    'Косил косой косой косой.',\n",
    "    'Глокая куздра штеко будланула бокра и куздрячит бокрёнка.',\n",
    "    'Сяпала Калуша с Калушатами по напушке.',\n",
    "    'Пирожки поставлены в печь, мама любит печь.',\n",
    "    'Ведро дало течь, вода стала течь.',\n",
    "    'Три да три, будет дырка.',\n",
    "    'Три да три, будет шесть.',\n",
    "    'Сорок сорок'\n",
    "]\n",
    "test_sentences_tokenized = tokenize_corpus(test_sentences, min_token_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 72.38it/s]                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбранное устройство: cuda\n",
      "мама-NOUN мыла-VERB раму-NOUN\n",
      "\n",
      "косил-VERB косой-NOUN косой-NOUN косой-NOUN\n",
      "\n",
      "глокая-ADJ куздра-NOUN штеко-NOUN будланула-VERB бокра-NOUN и-CCONJ куздрячит-VERB бокрёнка-NOUN\n",
      "\n",
      "сяпала-VERB калуша-NOUN с-ADP калушатами-NOUN по-ADP напушке-NOUN\n",
      "\n",
      "пирожки-NOUN поставлены-VERB в-ADP печь-NOUN мама-NOUN любит-VERB печь-NOUN\n",
      "\n",
      "ведро-ADV дало-VERB течь-NOUN вода-NOUN стала-VERB течь-NOUN\n",
      "\n",
      "три-NUM да-CCONJ три-NUM будет-AUX дырка-NOUN\n",
      "\n",
      "три-NUM да-CCONJ три-NUM будет-AUX шесть-NUM\n",
      "\n",
      "сорок-NOUN сорок-NOUN\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sent_tokens, sent_tags in zip(test_sentences_tokenized, sentence_level_pos_tagger(test_sentences)):\n",
    "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применение полученных теггеров и сравнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'single_token_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f01bd71cec50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msingle_token_pos_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPOSTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_token_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUNIQUE_TAGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SENT_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_ORIG_TOKEN_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msentence_level_pos_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPOSTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_level_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUNIQUE_TAGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SENT_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_ORIG_TOKEN_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'single_token_model' is not defined"
     ]
    }
   ],
   "source": [
    "single_token_pos_tagger = POSTagger(single_token_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "sentence_level_pos_tagger = POSTagger(sentence_level_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    'Мама мыла раму.',\n",
    "    'Косил косой косой косой.',\n",
    "    'Глокая куздра штеко будланула бокра и куздрячит бокрёнка.',\n",
    "    'Сяпала Калуша с Калушатами по напушке.',\n",
    "    'Пирожки поставлены в печь, мама любит печь.',\n",
    "    'Ведро дало течь, вода стала течь.',\n",
    "    'Три да три, будет дырка.',\n",
    "    'Три да три, будет шесть.',\n",
    "    'Сорок сорок'\n",
    "]\n",
    "test_sentences_tokenized = tokenize_corpus(test_sentences, min_token_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 113.53it/s]                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбранное устройство: cuda\n",
      "мама-NOUN мыла-VERB раму-NOUN\n",
      "\n",
      "косил-NOUN косой-NOUN косой-NOUN косой-NOUN\n",
      "\n",
      "глокая-ADJ куздра-NOUN штеко-ADJ будланула-VERB бокра-NOUN и-CCONJ куздрячит-VERB бокрёнка-NOUN\n",
      "\n",
      "сяпала-VERB калуша-NOUN с-ADP калушатами-NOUN по-ADP напушке-NOUN\n",
      "\n",
      "пирожки-NOUN поставлены-VERB в-ADP печь-VERB мама-NOUN любит-VERB печь-VERB\n",
      "\n",
      "ведро-ADV дало-VERB течь-VERB вода-NOUN стала-VERB течь-VERB\n",
      "\n",
      "три-NUM да-CCONJ три-NUM будет-VERB дырка-NOUN\n",
      "\n",
      "три-NUM да-CCONJ три-NUM будет-VERB шесть-NUM\n",
      "\n",
      "сорок-NOUN сорок-NOUN\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sent_tokens, sent_tags in zip(test_sentences_tokenized, single_token_pos_tagger(test_sentences)):\n",
    "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 111.45it/s]                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбранное устройство: cuda\n",
      "мама-NOUN мыла-VERB раму-NOUN\n",
      "\n",
      "косил-VERB косой-ADJ косой-ADJ косой-NOUN\n",
      "\n",
      "глокая-ADJ куздра-NOUN штеко-NOUN будланула-VERB бокра-NOUN и-CCONJ куздрячит-VERB бокрёнка-NOUN\n",
      "\n",
      "сяпала-VERB калуша-NOUN с-ADP калушатами-NOUN по-ADP напушке-NOUN\n",
      "\n",
      "пирожки-NOUN поставлены-VERB в-ADP печь-VERB мама-NOUN любит-VERB печь-VERB\n",
      "\n",
      "ведро-NOUN дало-VERB течь-VERB вода-NOUN стала-VERB течь-VERB\n",
      "\n",
      "три-NUM да-CCONJ три-NUM будет-AUX дырка-NOUN\n",
      "\n",
      "три-NUM да-CCONJ три-NUM будет-AUX шесть-NUM\n",
      "\n",
      "сорок-NOUN сорок-NOUN\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sent_tokens, sent_tags in zip(test_sentences_tokenized, sentence_level_pos_tagger(test_sentences)):\n",
    "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Свёрточный модуль своими руками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.weight = nn.Parameter(torch.randn(in_channels * kernel_size, out_channels) / (in_channels * kernel_size),\n",
    "                                   requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_channels), requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"x - BatchSize x InChannels x SequenceLen\"\"\"\n",
    "\n",
    "        batch_size, src_channels, sequence_len = x.shape        \n",
    "        if self.padding > 0:\n",
    "            pad = x.new_zeros(batch_size, src_channels, self.padding)\n",
    "            x = torch.cat((pad, x, pad), dim=-1)\n",
    "            sequence_len = x.shape[-1]\n",
    "\n",
    "        chunks = []\n",
    "        chunk_size = sequence_len - self.kernel_size + 1\n",
    "        for offset in range(self.kernel_size):\n",
    "            chunks.append(x[:, :, offset:offset + chunk_size])\n",
    "\n",
    "        in_features = torch.cat(chunks, dim=1)  # BatchSize x InChannels * KernelSize x ChunkSize\n",
    "        in_features = in_features.permute(0, 2, 1)  # BatchSize x ChunkSize x InChannels * KernelSize\n",
    "        out_features = torch.bmm(in_features, self.weight.unsqueeze(0).expand(batch_size, -1, -1)) + self.bias.unsqueeze(0).unsqueeze(0)\n",
    "        out_features = out_features.permute(0, 2, 1)  # BatchSize x OutChannels x ChunkSize\n",
    "        return out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров 84882\n"
     ]
    }
   ],
   "source": [
    "sentence_level_model_my_conv = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
    "                                                      single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d),\n",
    "                                                      context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d))\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model_my_conv.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбранное устройство: cuda\n",
      "Эпоха 0\n",
      "Эпоха: 501 итераций, 129.90 сек\n",
      "Среднее значение функции потерь на обучении 0.07622694853923753\n",
      "Среднее значение функции потерь на валидации 0.018297371851040585\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 1\n",
      "Эпоха: 501 итераций, 129.31 сек\n",
      "Среднее значение функции потерь на обучении 0.020714238068523877\n",
      "Среднее значение функции потерь на валидации 0.015549330317442961\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 2\n",
      "Эпоха: 501 итераций, 129.24 сек\n",
      "Среднее значение функции потерь на обучении 0.018027356256937195\n",
      "Среднее значение функции потерь на валидации 0.013457047960508874\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 3\n",
      "Эпоха: 501 итераций, 130.54 сек\n",
      "Среднее значение функции потерь на обучении 0.016559043397268134\n",
      "Среднее значение функции потерь на валидации 0.013518566689868965\n",
      "\n",
      "Эпоха 4\n",
      "Эпоха: 501 итераций, 130.10 сек\n",
      "Среднее значение функции потерь на обучении 0.015718446603926\n",
      "Среднее значение функции потерь на валидации 0.012259186300827136\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 5\n",
      "Эпоха: 501 итераций, 129.49 сек\n",
      "Среднее значение функции потерь на обучении 0.01506318770415947\n",
      "Среднее значение функции потерь на валидации 0.012300227897813414\n",
      "\n",
      "Эпоха 6\n",
      "Эпоха: 501 итераций, 130.41 сек\n",
      "Среднее значение функции потерь на обучении 0.014656914455700895\n",
      "Среднее значение функции потерь на валидации 0.011970557369254899\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 7\n",
      "Эпоха: 501 итераций, 129.22 сек\n",
      "Среднее значение функции потерь на обучении 0.014238949589981528\n",
      "Среднее значение функции потерь на валидации 0.011139537387592073\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 8\n",
      "Эпоха: 501 итераций, 129.17 сек\n",
      "Среднее значение функции потерь на обучении 0.013898625800724396\n",
      "Среднее значение функции потерь на валидации 0.011458761713439874\n",
      "\n",
      "Эпоха 9\n",
      "Эпоха: 501 итераций, 128.58 сек\n",
      "Среднее значение функции потерь на обучении 0.013547219125974083\n",
      "Среднее значение функции потерь на валидации 0.01109452576533255\n",
      "Новая лучшая модель!\n",
      "\n",
      "CPU times: user 22min 54s, sys: 696 ms, total: 22min 54s\n",
      "Wall time: 22min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "best_val_loss, best_sentence_level_model_my_conv = train_eval_loop(sentence_level_model_my_conv, train_dataset, test_dataset,\n",
    "    F.cross_entropy,\n",
    "    lr=5e-3,\n",
    "    epoch_n=10,\n",
    "    batch_size=64,\n",
    "    device='cuda',\n",
    "    early_stopping_patience=5,\n",
    "    max_batches_per_epoch_train=500,\n",
    "    max_batches_per_epoch_val=100,\n",
    "    lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2, factor=0.5, verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1525.4375 [00:00<01:02, 24.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбранное устройство: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1526it [00:59, 25.54it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на обучении 0.009719030931591988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 3/205.75 [00:00<00:08, 23.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   9136391\n",
      "         ADJ       0.93      0.94      0.93     85589\n",
      "         ADP       1.00      0.99      1.00     81963\n",
      "         ADV       0.92      0.91      0.91     44101\n",
      "         AUX       0.87      0.96      0.91      7522\n",
      "       CCONJ       0.94      0.98      0.96     30432\n",
      "         DET       0.92      0.92      0.92     21968\n",
      "        INTJ       0.80      0.41      0.54        78\n",
      "        NOUN       0.98      0.96      0.97    214497\n",
      "         NUM       0.96      0.96      0.96     13746\n",
      "        PART       0.96      0.88      0.92     26651\n",
      "        PRON       0.95      0.92      0.94     38438\n",
      "       PROPN       0.93      0.97      0.95     32401\n",
      "       PUNCT       1.00      1.00      1.00    157989\n",
      "       SCONJ       0.89      0.92      0.90     16219\n",
      "         SYM       1.00      1.00      1.00       840\n",
      "        VERB       0.94      0.97      0.95     97670\n",
      "           X       0.91      0.66      0.76       375\n",
      "\n",
      "    accuracy                           1.00  10006870\n",
      "   macro avg       0.94      0.91      0.92  10006870\n",
      "weighted avg       1.00      1.00      1.00  10006870\n",
      "\n",
      "\n",
      "Выбранное устройство: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/205.75 [00:08<00:00, 25.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на валидации 0.01101172436028719\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   1231232\n",
      "         ADJ       0.92      0.93      0.93     11222\n",
      "         ADP       1.00      0.99      1.00     10585\n",
      "         ADV       0.92      0.89      0.91      6165\n",
      "         AUX       0.86      0.96      0.91      1106\n",
      "       CCONJ       0.94      0.98      0.96      4410\n",
      "         DET       0.90      0.91      0.90      3085\n",
      "        INTJ       1.00      0.27      0.43        11\n",
      "        NOUN       0.97      0.96      0.97     27974\n",
      "         NUM       0.95      0.95      0.95      1829\n",
      "        PART       0.96      0.88      0.92      3877\n",
      "        PRON       0.94      0.91      0.93      5598\n",
      "       PROPN       0.92      0.96      0.94      4438\n",
      "       PUNCT       1.00      1.00      1.00     22694\n",
      "       SCONJ       0.86      0.91      0.88      2258\n",
      "         SYM       1.00      1.00      1.00        53\n",
      "        VERB       0.93      0.97      0.95     13078\n",
      "           X       0.93      0.81      0.87       105\n",
      "\n",
      "    accuracy                           1.00   1349720\n",
      "   macro avg       0.95      0.90      0.91   1349720\n",
      "weighted avg       1.00      1.00      1.00   1349720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pred = predict_with_model(best_sentence_level_model_my_conv, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(best_sentence_level_model_my_conv, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
