{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from random import sample\n",
    "\n",
    "from simcube.pipeline import init_random_seed, copy_data_to_device\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "with open('../../data/raw/author_quotes.txt') as input_file:\n",
    "    quotes = input_file.read()[:-1].split('\\n')\n",
    "    quotes = [' ' + line for line in quotes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' If you live to be a hundred, I want to live to be a hundred minus one day so I never have to live without you.',\n",
       " \" Promise me you'll always remember: You're braver than you believe, and stronger than you seem, and smarter than you think.\",\n",
       " ' Did you ever stop to think, and forget to start again?',\n",
       " ' Organizing is what you do before you do something, so that when you do it, it is not all mixed up.',\n",
       " ' Weeds are flowers too, once you get to know them.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(set(''.join(quotes)))\n",
    "token_to_id = {token: idx for idx, token in enumerate(tokens)}\n",
    "num_tokens = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36165"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "409"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(map(len, quotes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Quotes length distribution')\n",
    "plt.hist(list(map(len, quotes)), bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_tokens: 85\n"
     ]
    }
   ],
   "source": [
    "#all unique characters go here\n",
    "tokens = list(set(''.join(quotes)))\n",
    "\n",
    "num_tokens = len(tokens)\n",
    "print ('num_tokens:', num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {token: idx for idx, token in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(data, token_to_id, max_len=None, dtype='int32', batch_first = True):\n",
    "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len, data))\n",
    "    data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        line_ix = [token_to_id[char] for char in data[i]]\n",
    "        data_ix[i, :len(line_ix[:max_len])] = line_ix[:max_len]\n",
    "        \n",
    "    if not batch_first: # convert [batch, time] into [time, batch]\n",
    "        data_ix = np.transpose(data_ix)\n",
    "\n",
    "    return data_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " If you live to be a hundred, I want to live to be a hundred minus one day so I never have to live without you.\n",
      " In the beginning, everybody that gets to work with me, thinks I'm nice. But three weeks later, they hear a bell ringing. Then they realise I meant everything I said during that first week. It's not my fault people are not taking me serious from the first moment.\n",
      " I tend to get comfortable with the dialogue and find out who the person is in the script and try to hit that. People are sort of independent of their occupations and their pastimes. You don't play a politician or a fireman or a cowboy - you just play a person.\n",
      " 'Tis love that makes the world go round, my baby.\n",
      " Some of these kids just don't plain know how good they are: how smart and how much they have to say. You can tell them. You can shine that light on them, one human interaction at a time.\n",
      " I was able to realize that I definitely want to make sure that I use my voice, as it gets bigger and bigger, in the world for good.\n",
      " It seems, in fact, as though the second half of a man's life is made up of nothing, but the habits he has accumulated during the first half.\n",
      " The best and most beautiful things in the world cannot be seen or even touched - they must be felt with the heart.\n",
      " I never wanted to be an actor. I got stuck in it and kind of liked what I was doing.\n",
      " Listening is more important than talking. Just hit your mark and believe what you say. Just listen to people and react to what they are saying.\n",
      " Depression begins with disappointment. When disappointment festers in our soul, it leads to discouragement.\n",
      " The easiest thing to be in the world is you. The most difficult thing to be is what other people want you to be. Don't let them put you in that position.\n",
      " Think twice before burdening a friend with a secret.\n",
      " Religion is what keeps the poor from murdering the rich.\n",
      " Acting is a tough industry. There are a lot of kids out there at drama schools and not a lot of money about, especially as the arts are being cut.\n",
      " I promised never to let the Rwandan Genocide die because I knew the Rwandans didn't have much power internationally and certainly didn't have the resources. I felt it was my duty having witnessed it, and having stayed to witness it, that I had to talk about it and keep it going.\n",
      " Dolce & Gabbana is like our child. The editing of a collection before a show is a tough call, as we would like to show everything!\n",
      " I get meals when I can, or I have protein shakes.\n",
      " I'm from Hollywood; I'm too dumb to be nervous about New York.\n",
      "[[42  9 15 ... 42 42 42]\n",
      " [42  9 61 ... 42 42 42]\n",
      " [42  9 42 ... 42 42 42]\n",
      " ...\n",
      " [42 11 73 ... 42 42 42]\n",
      " [42  9 42 ... 42 42 42]\n",
      " [42  9  2 ... 42 42 42]]\n"
     ]
    }
   ],
   "source": [
    "#Example: cast 4 names to matrices, pad with zeros\n",
    "print('\\n'.join(quotes[::2000]))\n",
    "print(to_matrix(quotes[::2000], token_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 350\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(char_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n",
    "    '''\n",
    "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
    "    :param max_length: maximum output length, including seed_phrase\n",
    "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
    "                        smaller temperature converges to the single most likely output\n",
    "    '''\n",
    "    \n",
    "    x_sequence = [token_to_id[token] for token in seed_phrase]\n",
    "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n",
    "    hid_state = char_rnn.initial_state(batch_size=1)\n",
    "    \n",
    "    #feed the seed phrase, if any\n",
    "    for i in range(len(seed_phrase) - 1):\n",
    "        hid_state, _ = char_rnn(x_sequence[:, i], hid_state)\n",
    "    \n",
    "    #start generating\n",
    "    for _ in range(max_length - len(seed_phrase)):\n",
    "        hid_state, logp_next = char_rnn(x_sequence[:, -1], hid_state)\n",
    "        p_next = F.softmax(logp_next / temperature, dim=-1).data.numpy()[0]\n",
    "        \n",
    "        # sample next token and push it back into x_sequence\n",
    "        next_ix = np.random.choice(len(tokens), p=p_next)\n",
    "        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
    "        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
    "        \n",
    "    return ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNNLoop(nn.Module):\n",
    "    def __init__(self, num_tokens=num_tokens, emb_size=16, rnn_num_units=64, dropout=0.3):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_tokens, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rnn = nn.RNN(emb_size, rnn_num_units, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_num_units, num_tokens)\n",
    "        \n",
    "    def forward(self, src: Tensor) -> Tuple[Tensor]:\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        next_logits = self.fc(hidden)\n",
    "        next_logp = F.log_softmax(next_logits, dim=-1)\n",
    "        return next_logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNNLoop(emb_size=32)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNNLoop(\n",
       "  (embedding): Embedding(85, 32)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (rnn): RNN(32, 64, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=85, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров 14517\n"
     ]
    }
   ],
   "source": [
    "print('Количество параметров', sum(np.product(t.shape) for t in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: Input tensor must have same size as output tensor apart from the specified dimension at /pytorch/aten/src/THC/generic/THCTensorScatterGather.cu:27",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: Input tensor must have same size as output tensor apart from the specified dimension at /pytorch/aten/src/THC/generic/THCTensorScatterGather.cu:27"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(500):\n",
    "    batch_ix = to_matrix(sample(quotes, BATCH_SIZE), token_to_id, max_len=MAX_LENGTH)\n",
    "    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
    "    batch_ix = copy_data_to_device(batch_ix, device)\n",
    "    \n",
    "    logp_seq = model(batch_ix)\n",
    "    \n",
    "    # compute loss\n",
    "    predictions_logp = logp_seq[:, :-1]\n",
    "    actual_next_tokens = batch_ix[:, 1:]\n",
    "        \n",
    "    loss = -torch.mean(torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:,:,None]))\n",
    "    \n",
    "#     # train with backprop\n",
    "#     loss.backward()\n",
    "#     opt.step()\n",
    "#     opt.zero_grad()\n",
    "    \n",
    "#     history.append(loss.data.cpu().numpy())\n",
    "#     if (i + 1) % 100 == 0:\n",
    "#         clear_output(True)\n",
    "#         plt.plot(history, label='loss')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "assert np.mean(history[:25]) > np.mean(history[-25:]), \"RNN didn't converge.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(char_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n",
    "    '''\n",
    "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
    "    :param max_length: maximum output length, including seed_phrase\n",
    "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
    "                        smaller temperature converges to the single most likely output\n",
    "    '''\n",
    "    \n",
    "    x_sequence = [token_to_id[token] for token in seed_phrase]\n",
    "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n",
    "    \n",
    "    #feed the seed phrase, if any\n",
    "    for i in range(len(seed_phrase) - 1):\n",
    "        hid_state, _ = char_rnn(x_sequence[:, i])\n",
    "    \n",
    "    #start generating\n",
    "    for _ in range(max_length - len(seed_phrase)):\n",
    "        hid_state, logp_next = char_rnn(x_sequence[:, -1], hid_state)\n",
    "        p_next = F.softmax(logp_next / temperature, dim=-1).data.numpy()[0]\n",
    "        \n",
    "        # sample next token and push it back into x_sequence\n",
    "        next_ix = np.random.choice(len(tokens), p=p_next)\n",
    "        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
    "        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
    "        \n",
    "    return ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'hid_state' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-cb99b8bfdae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-0dc8ef862b83>\u001b[0m in \u001b[0;36mgenerate_sample\u001b[0;34m(char_rnn, seed_phrase, max_length, temperature)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#start generating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_phrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mhid_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogp_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mp_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogp_next\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'hid_state' referenced before assignment"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(generate_sample(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sequence = torch.tensor([], dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], dtype=torch.int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sequence[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-8f45d17a44e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlogp_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mp_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogp_next\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# sample next token and push it back into x_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/simcube/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-7731425b4228>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#         assert isinstance(x, Variable) and isinstance(x.data, torch.LongTensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mh_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mnext_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhid_to_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mnext_logp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/simcube/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/simcube/venv/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/projects/simcube/venv/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for _ in range(MAX_LENGTH):\n",
    "    logp_next = model(x_sequence)\n",
    "    p_next = F.softmax(logp_next / temperature, dim=-1).data.numpy()[0]\n",
    "\n",
    "    # sample next token and push it back into x_sequence\n",
    "    next_ix = np.random.choice(len(tokens), p=p_next)\n",
    "    next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
    "    x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
    "\n",
    "print(''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
