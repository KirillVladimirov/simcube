{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from pymongo import MongoClient\n",
    "import artm\n",
    "import click\n",
    "import mlflow\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = os.path.join(\"..\", \"..\", \"data\", \"raw\")\n",
    "processed_data_path = os.path.join(\"..\", \"..\", \"data\", \"processed\")\n",
    "models_path = os.path.join(\"..\", \"..\", \"models\")\n",
    "experiments_path = os.path.join(\"..\", \"..\", \"experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание файла с данными для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-02 00:41:02,325 : INFO : Loading main dataset...\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Loading main dataset...\")\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.publicru_test\n",
    "collection = db.documents_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = collection.find({}).skip(1000).limit(1)\n",
    "document = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_para = 5\n",
    "max_sent = 10\n",
    "\n",
    "text, bigrams, trigrams = [], [], []\n",
    "for j, para in enumerate(document[\"t_body\"][0]):\n",
    "    for i, sent in enumerate(para):\n",
    "        bigrams += list(nltk.bigrams(sent))\n",
    "        trigrams += list(nltk.trigrams(sent))\n",
    "        text += sent\n",
    "        if i > max_sent:\n",
    "            break\n",
    "    if j > max_para:\n",
    "        break\n",
    "bigrams = [\"!\".join(b) for b in bigrams]\n",
    "trigrams = [\"!\".join(t) for t in trigrams]\n",
    "#             title = document['t_title'][0] if document['t_title'] else []\n",
    "title = document['t_title'][0][0][0] if document['t_title'][0][0] else []\n",
    "parts = [f\"{document['_id']}\"]\n",
    "parts += ['|@title']  + title\n",
    "parts += ['|@text']  + text\n",
    "parts += ['|@bigrams']  + bigrams\n",
    "parts += ['|@trigrams']  + trigrams\n",
    "post = ' '.join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5e7854ecc59124ce04bed321 |@title чат_S помощь_S |@text аукцион_S рубрика_S вести_V информационный_A центр_S ъ рисунок_S сергей_S голосов_S покупатель_S любить_V общаться_V интернет_S магазин_S данные_S последний_A опрос_S проводить_V facebook половина_S потребитель_S предпочитать_V онлайн_ADV покупка_S сайт_S функция_S чат_S вывод_S коммуникация_S мессенджер_S набирать_V сила_S собираться_V замедляться_V будущее_S респондент_S пользоваться_V мессенджер_S коммерческий_A цель_S посылать_V сообщение_S ожидать_V активный_A переписка_S следующий_A использование_S приложение_S переписка_S являться_V прерогатива_S молодой_A миллениал_S предпочитать_V мессенджер_S звонок_S почта_S отставать_V родитель_S поколение_S х_S практически_ADV вровень_ADV идти_V бебибумер_S рождаться_V мировой_A война_S покупатель_S любить_V пользоваться_V услуга_S обратный_A связь_S быстрый_A отклик_S спрашивать_V товар_S услуга_S местонахождение_S часы_S работа_S магазин_S взаимодействие_S онлайн_ADV магазин_S мессенджер_S клиент_S оставлять_V отзыв_S публиковать_V фото_S покупка_S подготовка_S материал_S использовать_V информация_S die welt marketwatch busi insid pbs org reuter guardian |@bigrams рубрика_S!вести_V вести_V!информационный_A информационный_A!центр_S центр_S!ъ рисунок_S!сергей_S сергей_S!голосов_S покупатель_S!любить_V любить_V!общаться_V общаться_V!интернет_S интернет_S!магазин_S данные_S!последний_A последний_A!опрос_S опрос_S!проводить_V проводить_V!facebook facebook!половина_S половина_S!потребитель_S потребитель_S!предпочитать_V предпочитать_V!онлайн_ADV онлайн_ADV!покупка_S покупка_S!сайт_S сайт_S!функция_S функция_S!чат_S вывод_S!коммуникация_S коммуникация_S!мессенджер_S мессенджер_S!набирать_V набирать_V!сила_S сила_S!собираться_V собираться_V!замедляться_V замедляться_V!будущее_S респондент_S!пользоваться_V пользоваться_V!мессенджер_S мессенджер_S!коммерческий_A коммерческий_A!цель_S цель_S!посылать_V посылать_V!сообщение_S ожидать_V!активный_A активный_A!переписка_S переписка_S!следующий_A использование_S!приложение_S приложение_S!переписка_S переписка_S!являться_V являться_V!прерогатива_S прерогатива_S!молодой_A миллениал_S!предпочитать_V предпочитать_V!мессенджер_S мессенджер_S!звонок_S звонок_S!почта_S отставать_V!родитель_S родитель_S!поколение_S поколение_S!х_S х_S!практически_ADV практически_ADV!вровень_ADV вровень_ADV!идти_V идти_V!бебибумер_S бебибумер_S!рождаться_V рождаться_V!мировой_A мировой_A!война_S покупатель_S!любить_V любить_V!пользоваться_V пользоваться_V!услуга_S услуга_S!обратный_A обратный_A!связь_S связь_S!быстрый_A быстрый_A!отклик_S отклик_S!спрашивать_V спрашивать_V!товар_S товар_S!услуга_S услуга_S!местонахождение_S местонахождение_S!часы_S часы_S!работа_S работа_S!магазин_S взаимодействие_S!онлайн_ADV онлайн_ADV!магазин_S магазин_S!мессенджер_S мессенджер_S!клиент_S клиент_S!оставлять_V оставлять_V!отзыв_S отзыв_S!публиковать_V публиковать_V!фото_S фото_S!покупка_S подготовка_S!материал_S материал_S!использовать_V использовать_V!информация_S информация_S!die die!welt welt!marketwatch marketwatch!busi busi!insid insid!pbs pbs!org org!reuter reuter!guardian |@trigrams рубрика_S!вести_V!информационный_A вести_V!информационный_A!центр_S информационный_A!центр_S!ъ рисунок_S!сергей_S!голосов_S покупатель_S!любить_V!общаться_V любить_V!общаться_V!интернет_S общаться_V!интернет_S!магазин_S данные_S!последний_A!опрос_S последний_A!опрос_S!проводить_V опрос_S!проводить_V!facebook проводить_V!facebook!половина_S facebook!половина_S!потребитель_S половина_S!потребитель_S!предпочитать_V потребитель_S!предпочитать_V!онлайн_ADV предпочитать_V!онлайн_ADV!покупка_S онлайн_ADV!покупка_S!сайт_S покупка_S!сайт_S!функция_S сайт_S!функция_S!чат_S вывод_S!коммуникация_S!мессенджер_S коммуникация_S!мессенджер_S!набирать_V мессенджер_S!набирать_V!сила_S набирать_V!сила_S!собираться_V сила_S!собираться_V!замедляться_V собираться_V!замедляться_V!будущее_S респондент_S!пользоваться_V!мессенджер_S пользоваться_V!мессенджер_S!коммерческий_A мессенджер_S!коммерческий_A!цель_S коммерческий_A!цель_S!посылать_V цель_S!посылать_V!сообщение_S ожидать_V!активный_A!переписка_S активный_A!переписка_S!следующий_A использование_S!приложение_S!переписка_S приложение_S!переписка_S!являться_V переписка_S!являться_V!прерогатива_S являться_V!прерогатива_S!молодой_A миллениал_S!предпочитать_V!мессенджер_S предпочитать_V!мессенджер_S!звонок_S мессенджер_S!звонок_S!почта_S отставать_V!родитель_S!поколение_S родитель_S!поколение_S!х_S поколение_S!х_S!практически_ADV х_S!практически_ADV!вровень_ADV практически_ADV!вровень_ADV!идти_V вровень_ADV!идти_V!бебибумер_S идти_V!бебибумер_S!рождаться_V бебибумер_S!рождаться_V!мировой_A рождаться_V!мировой_A!война_S покупатель_S!любить_V!пользоваться_V любить_V!пользоваться_V!услуга_S пользоваться_V!услуга_S!обратный_A услуга_S!обратный_A!связь_S обратный_A!связь_S!быстрый_A связь_S!быстрый_A!отклик_S быстрый_A!отклик_S!спрашивать_V отклик_S!спрашивать_V!товар_S спрашивать_V!товар_S!услуга_S товар_S!услуга_S!местонахождение_S услуга_S!местонахождение_S!часы_S местонахождение_S!часы_S!работа_S часы_S!работа_S!магазин_S взаимодействие_S!онлайн_ADV!магазин_S онлайн_ADV!магазин_S!мессенджер_S магазин_S!мессенджер_S!клиент_S мессенджер_S!клиент_S!оставлять_V клиент_S!оставлять_V!отзыв_S оставлять_V!отзыв_S!публиковать_V отзыв_S!публиковать_V!фото_S публиковать_V!фото_S!покупка_S подготовка_S!материал_S!использовать_V материал_S!использовать_V!информация_S использовать_V!информация_S!die информация_S!die!welt die!welt!marketwatch welt!marketwatch!busi marketwatch!busi!insid busi!insid!pbs insid!pbs!org pbs!org!reuter org!reuter!guardian'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus_file(collection, path_folder):\n",
    "    with open(os.path.join(experiments_path, path_folder, 'vowpal_wabbit_corpus.txt'), 'w') as the_file:\n",
    "        mongo_filter = {\n",
    "            \"t_title\": {\"$exists\": True}, \n",
    "            \"t_body\": {\"$exists\": True}, \n",
    "            \"words_count\": {\"$gt\": 400}\n",
    "        }\n",
    "        max_para = 5\n",
    "        max_sent = 10\n",
    "        for document in tqdm(\n",
    "                collection.find(mongo_filter, {\"_id\": 1, \"t_title\": 1, \"t_body\": 1}, no_cursor_timeout=True)):\n",
    "            text, bigrams, trigrams = [], [], []\n",
    "            for j, para in enumerate(document[\"t_body\"][0]):\n",
    "                for i, sent in enumerate(para):\n",
    "                    bigrams += list(nltk.bigrams(sent))\n",
    "                    trigrams += list(nltk.trigrams(sent))\n",
    "                    text += sent\n",
    "                    if i > max_sent:\n",
    "                        break\n",
    "                if j > max_para:\n",
    "                    break\n",
    "            bigrams = [\"!\".join(b) for b in bigrams]\n",
    "            trigrams = [\"!\".join(t) for t in trigrams]\n",
    "#             title = document['t_title'][0] if document['t_title'] else []\n",
    "            title = document['t_title'][0]\n",
    "            if title:\n",
    "                title = title[0][0] if title[0] else []\n",
    "            parts = [f\"{document['_id']}\"]\n",
    "            parts += ['|@title']  + title\n",
    "            parts += ['|@text']  + text\n",
    "            parts += ['|@bigrams']  + bigrams\n",
    "            parts += ['|@trigrams']  + trigrams\n",
    "            post = ' '.join(parts)\n",
    "            the_file.write(f\"{post}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_batch(path_folder):\n",
    "    if len(glob.glob(os.path.join(experiments_path, path_folder, 'batches', '*.batch'))) > 1:\n",
    "        logging.info(\"Remove old batches.\")\n",
    "        pth = Path(os.path.join(experiments_path, path_folder, 'batches'))\n",
    "        for child in pth.glob('*'):\n",
    "            if child.is_file():\n",
    "                child.unlink()\n",
    "        pth.rmdir()\n",
    "\n",
    "    logging.info(\"Generate batches files...\")\n",
    "    batch_vectorizer = artm.BatchVectorizer(\n",
    "        data_path=os.path.join(experiments_path, path_folder, 'vowpal_wabbit_corpus.txt'),\n",
    "        data_format='vowpal_wabbit',\n",
    "        batch_size=5000,\n",
    "        target_folder=os.path.join(experiments_path, path_folder, 'batches'),\n",
    "    )\n",
    "    logging.info(\"Gathering dictionary...\")\n",
    "    dictionary = batch_vectorizer.dictionary\n",
    "    dictionary.save_text(dictionary_path=os.path.join(experiments_path, path_folder, 'dictionary.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"multiclass_model_test_all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-02 00:45:41,451 : INFO : Original dataset size: 588456\n",
      "2020-04-02 00:45:42,324 : INFO : Working dataset size: 226590\n"
     ]
    }
   ],
   "source": [
    "# mongo_filter = {\"words_count\": {\"$gt\": 400}}\n",
    "mongo_filter = {\"words_count\": {\"$gt\": 400}}\n",
    "logging.info(f\"Original dataset size: {collection.count_documents({})}\")\n",
    "logging.info(f\"Working dataset size: {collection.count_documents(mongo_filter)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-02 00:45:45,338 : INFO : Creating corpus file in vw format...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cdf119aa74b4afeb647f452a3883f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-02 00:48:36,248 : INFO : Done.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Creating corpus file in vw format...\")\n",
    "Path(os.path.join(experiments_path, data_folder)).mkdir(parents=True, exist_ok=True)\n",
    "create_corpus_file(collection, data_folder)\n",
    "logging.info(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-02 00:48:36,258 : INFO : Preparing batches for artm model...\n",
      "2020-04-02 00:48:36,261 : INFO : Remove old batches.\n",
      "2020-04-02 00:48:37,693 : INFO : Generate batches files...\n",
      "2020-04-02 01:01:21,712 : INFO : Gathering dictionary...\n"
     ]
    },
    {
     "ename": "InvalidOperationException",
     "evalue": "Unable to serialize the message",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidOperationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f933c38b6039>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preparing batches for artm model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreparing_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finish.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5e7acc73f386>\u001b[0m in \u001b[0;36mpreparing_batch\u001b[0;34m(path_folder)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gathering dictionary...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiments_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dictionary.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/rbc_projects/rbcpro-ml-lab/venv/lib/python3.7/site-packages/artm/dictionary.py\u001b[0m in \u001b[0;36msave_text\u001b[0;34m(self, dictionary_path, encoding)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mstr\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0man\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdiciotnary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mdictionary_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_master\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             fout.write(u'name: {} num_items: {}\\n'.format(dictionary_data.name,\n",
      "\u001b[0;32m~/rbc_projects/rbcpro-ml-lab/venv/lib/python3.7/site-packages/artm/master_component.py\u001b[0m in \u001b[0;36mget_dictionary\u001b[0;34m(self, dictionary_name)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \"\"\"\n\u001b[1;32m    336\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetDictionaryArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mdictionary_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArtmRequestDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdictionary_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rbc_projects/rbcpro-ml-lab/venv/lib/python3.7/site-packages/artm/wrapper/api.py\u001b[0m in \u001b[0;36martm_api_call\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;31m# return result value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rbc_projects/rbcpro-ml-lab/venv/lib/python3.7/site-packages/artm/wrapper/api.py\u001b[0m in \u001b[0;36m_check_error\u001b[0;34m(self, error_code)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mexception_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mARTM_EXCEPTION_BY_CODE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexception_class\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidOperationException\u001b[0m: Unable to serialize the message"
     ]
    }
   ],
   "source": [
    "logging.info(\"Preparing batches for artm model...\")\n",
    "preparing_batch(data_folder)\n",
    "logging.info(\"Finish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah ../../experiments/multiclass_model_test_all/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batches(path_folder, min_df):\n",
    "    \"\"\"\n",
    "    path_folder: string, folder with vowpal_wabbit_corpus.txt and batches/\n",
    "    min_df: integer, minimal document frequency \n",
    "    \"\"\"\n",
    "    if len(glob.glob(os.path.join(experiments_path, path_folder, 'batches', '*.batch'))) < 1:\n",
    "        logging.info(\"Generating batches files...\")\n",
    "        bv = artm.BatchVectorizer(\n",
    "            data_path=os.path.join(experiments_path, path_folder, 'vowpal_wabbit_corpus.txt'),\n",
    "            data_format='vowpal_wabbit',\n",
    "            batch_size=2000,\n",
    "            target_folder=os.path.join(experiments_path, path_folder, 'batches'),\n",
    "        )\n",
    "        dictionary = bv.dictionary\n",
    "        dictionary.save_text(dictionary_path=os.path.join(experiments_path, path_folder, 'dictionary.txt'))\n",
    "    else:\n",
    "        logging.info(\"Loading batches files...\")\n",
    "        bv = artm.BatchVectorizer(\n",
    "            data_path=os.path.join(experiments_path, path_folder, 'batches'),\n",
    "            data_format='batches',\n",
    "        )\n",
    "        dictionary = artm.Dictionary()\n",
    "        dictionary.load_text(dictionary_path=os.path.join(experiments_path, path_folder, 'dictionary.txt'))\n",
    "\n",
    "    regex = \"\\d+\"\n",
    "    match = re.findall(regex, str(dictionary))\n",
    "    logging.info(f\"Original dictionary size: {int(match[-1])} words.\")\n",
    "    dictionary.filter(min_df=min_df, inplace=True)\n",
    "    match = re.findall(regex, str(dictionary))\n",
    "    logging.info(f\"Filtered dictionary size: {int(match[-1])} words.\")\n",
    "    return bv, dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic_names(topic_count=200, background_topic_count=20):\n",
    "    objective_topics = ['objective_topic_' + str(x) for x in range(0, topic_count - background_topic_count)]\n",
    "    background_topics = ['background_topic_' + str(x) for x in range(topic_count - background_topic_count, topic_count)]\n",
    "    all_topics = objective_topics + background_topics\n",
    "\n",
    "    return all_topics, objective_topics, background_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_measures(model):\n",
    "    logging.info('Sparsity Title Phi: {0:.3f}'.format(model.score_tracker['SparsityPhiTitleScore'].last_value))\n",
    "    logging.info('Sparsity Text Phi: {0:.3f}'.format(model.score_tracker['SparsityPhiTextScore'].last_value))\n",
    "    logging.info('Sparsity Bigrams Phi: {0:.3f}'.format(model.score_tracker['SparsityPhiBigramsScore'].last_value))\n",
    "    logging.info('Sparsity Trigrams Phi: {0:.3f}'.format(model.score_tracker['SparsityPhiTrigramsScore'].last_value))\n",
    "    logging.info('Sparsity Theta: {0:.3f}'.format(model.score_tracker['SparsityThetaScore'].last_value))\n",
    "    logging.info('Kernel title contrast: {0:.3f}'.format(model.score_tracker['TopicKernelTitleScore'].last_average_contrast))\n",
    "    logging.info('Kernel text contrast: {0:.3f}'.format(model.score_tracker['TopicKernelTextScore'].last_average_contrast))\n",
    "    logging.info('Kernel bigrams contrast: {0:.3f}'.format(model.score_tracker['TopicKernelBigramsScore'].last_average_contrast))\n",
    "    logging.info('Kernel trigrams contrast: {0:.3f}'.format(model.score_tracker['TopicKernelTrigramsScore'].last_average_contrast))\n",
    "    logging.info('Kernel title purity: {0:.3f}'.format(model.score_tracker['TopicKernelTitleScore'].last_average_purity))\n",
    "    logging.info('Kernel text purity: {0:.3f}'.format(model.score_tracker['TopicKernelTextScore'].last_average_purity))\n",
    "    logging.info('Kernel bigrams purity: {0:.3f}'.format(model.score_tracker['TopicKernelBigramsScore'].last_average_purity))\n",
    "    logging.info('Kernel trigrams purity: {0:.3f}'.format(model.score_tracker['TopicKernelTrigramsScore'].last_average_purity))\n",
    "    logging.info('Perplexity: {0:.3f}'.format(model.score_tracker['PerplexityScore'].last_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name():\n",
    "    return str(uuid.uuid1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_log_metrics(model):\n",
    "    mlflow.log_metrics({\n",
    "        \"DeccorPhi\": model.regularizers['DeccorPhi'].tau,\n",
    "        \"SmoothPhi\": model.regularizers['SmoothPhi'].tau,\n",
    "        \"SmoothTheta\": model.regularizers['SmoothTheta'].tau,\n",
    "        \"SparsePhi\": model.regularizers['SparsePhi'].tau,\n",
    "        \"SparseTheta\": model.regularizers['SparseTheta'].tau,\n",
    "        \"SparsityPhiTitleScore\": model.score_tracker['SparsityPhiTitleScore'].last_value,\n",
    "        \"SparsityPhiTextScore\": model.score_tracker['SparsityPhiTextScore'].last_value,\n",
    "        \"SparsityPhiBigramsScore\": model.score_tracker['SparsityPhiBigramsScore'].last_value,\n",
    "        \"SparsityPhiTrigramsScore\": model.score_tracker['SparsityPhiTrigramsScore'].last_value,\n",
    "        \"SparsityThetaScore\": model.score_tracker['SparsityThetaScore'].last_value,\n",
    "        \"KernelContrastTitleScore\": model.score_tracker['TopicKernelTitleScore'].last_average_contrast,\n",
    "        \"KernelContrastTextScore\": model.score_tracker['TopicKernelTextScore'].last_average_contrast,\n",
    "        \"KernelContrastBigramsScore\": model.score_tracker['TopicKernelBigramsScore'].last_average_contrast,\n",
    "        \"KernelContrastTrigramsScore\": model.score_tracker['TopicKernelTrigramsScore'].last_average_contrast,\n",
    "        \"TopicPurityTitleScore\": model.score_tracker['TopicKernelTitleScore'].last_average_purity,\n",
    "        \"TopicPurityTextScore\": model.score_tracker['TopicKernelTextScore'].last_average_purity,\n",
    "        \"TopicPurityBigramsScore\": model.score_tracker['TopicKernelBigramsScore'].last_average_purity,\n",
    "        \"TopicPurityTrigramsScore\": model.score_tracker['TopicKernelTrigramsScore'].last_average_purity,\n",
    "        \"PerplexityScore\": model.score_tracker['PerplexityScore'].last_value,\n",
    "    }, step=model.num_phi_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_step(i, model, batch_vectorizer, step_size, dataset_name):\n",
    "    model_name = generate_name()\n",
    "    logging.info(model_name)\n",
    "\n",
    "    for _ in tqdm(range(step_size)):\n",
    "        model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=1)\n",
    "        mlflow_log_metrics(model)\n",
    "    print_measures(model)\n",
    "\n",
    "    Path(os.path.join(experiments_path, 'models')).mkdir(parents=True, exist_ok=True)\n",
    "    model_dir_name = os.path.join(experiments_path, 'models', f\"{model_name}\")\n",
    "    model.dump_artm_model(model_dir_name)\n",
    "    mlflow.set_tag(f\"model_dump_{i}\", model_dir_name)\n",
    "    mlflow.log_artifacts(model_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"multiclass_model_test_all\"\n",
    "min_df = 6\n",
    "num_all_topics = 400\n",
    "num_background_topics = 20\n",
    "step_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bv, dictionary = load_batches(dataset_name, min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics, objective_topics, background_topics = create_topic_names(num_all_topics, num_background_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_artm = [\n",
    "        artm.PerplexityScore(name='PerplexityScore', dictionary=dictionary, class_ids=[\"@title\", \"@text\", \"@bigrams\", \"@trigrams\"]),\n",
    "        artm.SparsityPhiScore(name='SparsityPhiTitleScore', topic_names=objective_topics, class_id=\"@title\"),\n",
    "        artm.SparsityPhiScore(name='SparsityPhiTextScore', topic_names=objective_topics, class_id=\"@text\"),\n",
    "        artm.SparsityPhiScore(name='SparsityPhiBigramsScore', topic_names=objective_topics, class_id=\"@bigrams\"),\n",
    "        artm.SparsityPhiScore(name='SparsityPhiTrigramsScore', topic_names=objective_topics, class_id=\"@trigrams\"),\n",
    "        artm.SparsityThetaScore(name='SparsityThetaScore', topic_names=objective_topics),\n",
    "        artm.TopTokensScore(name='TopTokensTitleScore', num_tokens=20, topic_names=objective_topics, dictionary=dictionary, class_id=\"@title\"),\n",
    "        artm.TopTokensScore(name='TopTokensTextScore', num_tokens=20, topic_names=objective_topics, dictionary=dictionary, class_id=\"@text\"),\n",
    "        artm.TopTokensScore(name='TopTokensBigramsScore', num_tokens=20, topic_names=objective_topics, dictionary=dictionary, class_id=\"@bigrams\"),\n",
    "        artm.TopTokensScore(name='TopTokensTrigramsScore', num_tokens=20, topic_names=objective_topics, dictionary=dictionary, class_id=\"@trigrams\"),\n",
    "        artm.TopicKernelScore(name='TopicKernelTitleScore', class_id=\"@title\", probability_mass_threshold=0.25,\n",
    "                              topic_names=objective_topics, dictionary=dictionary),\n",
    "        artm.TopicKernelScore(name='TopicKernelTextScore', class_id=\"@text\", probability_mass_threshold=0.25,\n",
    "                              topic_names=objective_topics, dictionary=dictionary),\n",
    "        artm.TopicKernelScore(name='TopicKernelBigramsScore', class_id=\"@bigrams\", probability_mass_threshold=0.25,\n",
    "                              topic_names=objective_topics, dictionary=dictionary),\n",
    "        artm.TopicKernelScore(name='TopicKernelTrigramsScore', class_id=\"@trigrams\", probability_mass_threshold=0.25,\n",
    "                              topic_names=objective_topics, dictionary=dictionary),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizers_artm = [\n",
    "        artm.DecorrelatorPhiRegularizer(name='DeccorPhi', topic_names=objective_topics, gamma=0, tau=0),\n",
    "        artm.SmoothSparsePhiRegularizer(name='SparsePhi', topic_names=objective_topics, dictionary=dictionary, gamma=0,\n",
    "                                        tau=0),\n",
    "        artm.SmoothSparsePhiRegularizer(name='SmoothPhi', topic_names=background_topics, dictionary=dictionary, gamma=0,\n",
    "                                        tau=0),\n",
    "        artm.SmoothSparseThetaRegularizer(name='SparseTheta', topic_names=objective_topics, tau=0),\n",
    "        artm.SmoothSparseThetaRegularizer(name='SmoothTheta', topic_names=background_topics, tau=0),\n",
    "        artm.TopicSelectionThetaRegularizer(name='TopicSelectionTheta', topic_names=objective_topics, tau=0)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = artm.ARTM(\n",
    "        num_topics=num_all_topics,\n",
    "        topic_names=all_topics,\n",
    "        class_ids={'@title': 3.0, '@text': 1.0, \"@bigrams\": 2.0, \"@trigrams\": 4.0},\n",
    "        num_processors=mp.cpu_count() - 1,\n",
    "        num_document_passes=2,\n",
    "        regularizers=regularizers_artm,\n",
    "        scores=scores_artm,\n",
    "        dictionary=dictionary,\n",
    "        cache_theta=False,\n",
    "        seed=42,\n",
    "        show_progress_bars=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Run learning...\")\n",
    "mlflow.set_experiment(dataset_name)\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # этап 1 - сильная декорреляция + сглаживание\n",
    "    # Sparse < 0\n",
    "    # Smooth > 0\n",
    "    model.regularizers['DeccorPhi'].tau = 0.005\n",
    "    model.regularizers['SmoothPhi'].tau = 0.4\n",
    "    model.regularizers['SmoothTheta'].tau = 0.4\n",
    "    next_step(1, model, bv, step_size, dataset_name)\n",
    "\n",
    "    model.regularizers['DeccorPhi'].tau = 0.015\n",
    "    model.regularizers['SmoothPhi'].tau = 0.6\n",
    "    model.regularizers['SmoothTheta'].tau = 0.6\n",
    "    next_step(2, model, bv, step_size, dataset_name)\n",
    "\n",
    "    model.regularizers['DeccorPhi'].tau = 0.03\n",
    "    model.regularizers['SmoothPhi'].tau = 0.8\n",
    "    model.regularizers['SmoothTheta'].tau = 0.8\n",
    "    next_step(3, model, bv, step_size, dataset_name)\n",
    "\n",
    "    # этап 2 - подключение разреживания предметных, постепенное увеличение разреживания\n",
    "    # Sparse < 0\n",
    "    # Smooth > 0\n",
    "    model.regularizers['SparsePhi'].tau = -0.0001\n",
    "    model.regularizers['SparseTheta'].tau = -0.1\n",
    "    next_step(4, model, bv, step_size, dataset_name)\n",
    "\n",
    "    model.regularizers['SparsePhi'].tau = -0.0002\n",
    "    model.regularizers['SparseTheta'].tau = -0.2\n",
    "    next_step(5, model, bv, step_size, dataset_name)\n",
    "\n",
    "    model.regularizers['SparsePhi'].tau = -0.0003\n",
    "    model.regularizers['SparseTheta'].tau = -0.3\n",
    "    next_step(6, model, bv, step_size, dataset_name)\n",
    "\n",
    "    # этап 3\n",
    "    # Sparse < 0\n",
    "    # Smooth > 0\n",
    "    model.regularizers['SparsePhi'].tau = -0.0005\n",
    "    model.regularizers['SparseTheta'].tau = -0.4\n",
    "    next_step(7, model, bv, step_size, dataset_name)\n",
    "\n",
    "    model.regularizers['SparsePhi'].tau = -0.0006\n",
    "    model.regularizers['SparseTheta'].tau = -0.5\n",
    "    next_step(8, model, bv, step_size, dataset_name)\n",
    "\n",
    "    model.regularizers['SparsePhi'].tau = -0.0007\n",
    "    model.regularizers['SparseTheta'].tau = -0.6\n",
    "    next_step(9, model, bv, step_size, dataset_name)\n",
    "\n",
    "logging.info(\"Finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(\"topics_print.txt\",\"w\") as f:\n",
    "    topics_tokens = \"\"\n",
    "    for i, topic_name in enumerate(model.topic_names[:190]):\n",
    "        f.write(f\"topic_name: {i+1}\\n\\n\")\n",
    "\n",
    "        titles = \" \".join([word.split(\"_\")[0] for word in model.score_tracker['TopTokensTitleScore'].last_tokens[topic_name]])\n",
    "        f.write(f\"title keywords: {titles}\\n\\n\")\n",
    "       \n",
    "        texts = \" \".join([word.split(\"_\")[0] for word in model.score_tracker['TopTokensTextScore'].last_tokens[topic_name]])\n",
    "        f.write(f\"text keywords: {texts}\\n\\n\")\n",
    "\n",
    "        bigrams = []\n",
    "        for bigram in model.score_tracker['TopTokensBigramsScore'].last_tokens[topic_name]:\n",
    "            words = []\n",
    "            for word in bigram.split(\"!\"):\n",
    "                words.append(word.split(\"_\")[0])\n",
    "            bigrams.append(\"_\".join(words))\n",
    "        f.write(f\"bigram keywords: {' '.join(bigrams)}\\n\\n\")\n",
    "        \n",
    "        trigrams = []\n",
    "        for trigram in model.score_tracker['TopTokensTrigramsScore'].last_tokens[topic_name]:\n",
    "            words = []\n",
    "            for word in trigram.split(\"!\"):\n",
    "                words.append(word.split(\"_\")[0])\n",
    "            trigrams.append(\"_\".join(words))\n",
    "        f.write(f\"trigram keywords: {' '.join(trigrams)}\\n{'-'*100}\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
