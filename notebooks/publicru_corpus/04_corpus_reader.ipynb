{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.data import LazyLoader\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.util import AbstractLazySequence, LazyMap, LazyConcatenation\n",
    "from pymongo import MongoClient\n",
    "import bs4\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pymystem3 import Mystem\n",
    "import pymorphy2\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = os.path.join(\"..\", \"..\", \"data\", \"raw\")\n",
    "processed_data_path = os.path.join(\"..\", \"..\", \"data\", \"processed\")\n",
    "models_path = os.path.join(\"..\", \"..\", \"models\")\n",
    "experiments_path = os.path.join(\"..\", \"..\", \"experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MongoDBLazySequence(AbstractLazySequence):\n",
    "    def __init__(self, host='localhost', port=27017, db='test',\n",
    "        collection='corpus', field='text'):\n",
    "        self.conn = MongoClient(host, port)\n",
    "        self.collection = self.conn[db][collection]\n",
    "        self.field = field\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.collection.count_documents({})\n",
    "    \n",
    "    def iterate_from(self, start):\n",
    "        f = lambda d: d.get(self.field, '')\n",
    "        return iter(LazyMap(f, self.collection.find(projection={self.field: 1}, skip=start)))\n",
    "\n",
    "\n",
    "class MongoDBCorpusReader:\n",
    "    def __init__(self, word_tokenizer=TreebankWordTokenizer(),\n",
    "        sent_tokenizer=LazyLoader('tokenizers/punkt/PY3/english.pickle'),**kwargs):\n",
    "        self._seq = MongoDBLazySequence(**kwargs)\n",
    "        self._word_tokenize = word_tokenizer.tokenize\n",
    "        self._sent_tokenize = sent_tokenizer.tokenize\n",
    "\n",
    "    def text(self):\n",
    "        return self._seq\n",
    "    \n",
    "    def words(self):\n",
    "        return LazyConcatenation(LazyMap(self._word_tokenize, self.text()))\n",
    "    \n",
    "    def sents(self):\n",
    "        return LazyConcatenation(LazyMap(self._sent_tokenize, self.text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_reader = MongodbCorpusReader(('localhost', 27017), \"publicru_test\", \"documents_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582052"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3404"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_reader.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = MongoDBCorpusReader(db='publicru_test', collection='documents_collection', field='body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<body><p><b>Мы взяли две компании с сопоставимой выручкой и оценили влияние курса рубля на их финансы.', 'Первый участник - это производитель удобрений - «ФосАгро», который большую часть выручки получает от экспорта.', ...]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<body><p><b>Мы взяли две компании с сопоставимой выручкой и оценили влияние курса рубля на их финансы.', 'Первый участник - это производитель удобрений - «ФосАгро», который большую часть выручки получает от экспорта.', ...]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<', 'body', '>', '<', 'p', '>', '<', 'b', '>', 'Мы', ...]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<body><p><b>Мы взяли две компании с сопоставимой выручкой и оценили влияние курса рубля на их финансы. Первый участник - это производитель удобрений - «ФосАгро», который большую часть выручки получает от экспорта. Второй участник ориентирован на внутренний рынок - это компания «М.видео», продающая электронику и технику. В качестве начала отчета возьмем 2013 год, когда курс рубля к доллару оставался стабильным и находился в диапазоне 30 - 33,5.</b></p>\\n<p><img alt=\"\" src=\"325/1.jpg\" title=\"\"/></p>\\n</body>', '<body><p><b>Индексы потребительских цен на товары и услуги по Российской Федерации в апреле - июне 2016 г.:</b></p>\\n<p><img alt=\"\" src=\"21225/1.jpg\" title=\"\"/></p>\\n<p><b>За ценами на продукты следите на сайте www.rg.ru/sujet/3131</b></p>\\n</body>', ...]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582052"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reader.text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MongoDBLazySequence(AbstractLazySequence):\n",
    "    def __init__(self, host='localhost', port=27017, database='test', collection='corpus', field='text'):\n",
    "        self.conn = MongoClient(host, port)\n",
    "        self.collection = self.conn[db][collection]\n",
    "        self.field = field\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.collection.count_documents({})\n",
    "    \n",
    "    def iterate_from(self, start):\n",
    "        f = lambda d: d.get(self.field, '')\n",
    "        return iter(LazyMap(f, self.collection.find(projection={self.field: 1}, skip=start)))\n",
    "\n",
    "class MongodbCorpusReader(CorpusReader):\n",
    "    \"\"\"\n",
    "    MongodbCorpusReader работает с корпусом текстов в HTML формате и \n",
    "    предоставляет инструменты предварительной обработки \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, connections, database, collection):\n",
    "        self._seq = MongoDBLazySequence(**kwargs)\n",
    "        self.client = MongoClient(*connections)\n",
    "        self.db = self.client[database]\n",
    "        self.collection = self.db[collection]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.collection.count_documents({})\n",
    "    \n",
    "    def size(self):\n",
    "        return self.db.command(\"collstats\", self.collection.name, scale=1024*1024)[\"storageSize\"]\n",
    "            \n",
    "    def paras():\n",
    "        paras_tags = [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \"li\"]\n",
    "        for html in self.docs:\n",
    "            soup = bs4.BeautifulSoup(html, \"lxml\")\n",
    "            for element in soup.find_all(paras_tags)\n",
    "                yield element.text\n",
    "            soup.decompose()\n",
    "            \n",
    "    def describe(self):      \n",
    "        counts = FreqDist()\n",
    "        tokens = FreqDist()\n",
    "        \n",
    "        for para in self.paras():\n",
    "            count[\"paras\"] += 1\n",
    "            \n",
    "            for sent in para:\n",
    "                count[\"sents\"] += 1\n",
    "                \n",
    "                for word, tag in sent:\n",
    "                    counts[\"words\"] += 1\n",
    "                    tokens[word] += 1\n",
    "                    \n",
    "        n_docs = None\n",
    "        n_topics = None\n",
    "        \n",
    "        return {\n",
    "            \"n_docs\": n_docs,\n",
    "            \"n_topics\": n_topics,\n",
    "            \"paras\": counts[\"paras\"],\n",
    "            \"sents\": counts[\"sents\"],\n",
    "            \"words\": counts[\"words\"],\n",
    "            \"vocab\": len(tokens),\n",
    "            \"lexdiv\": float(counts[\"words\"]) / float(len(tokens)),\n",
    "            \"ppdoc\": float(counts[\"paras\"]) / float(n_docs),\n",
    "            \"sppar\": float(counts[\"sents\"]) / float(counts[\"paras\"]),\n",
    "        }\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MongoDBLazySequence(AbstractLazySequence):\n",
    "    def __init__(self, host='localhost', port=27017, db='test', collection='corpus', field='text'):\n",
    "        self.conn = MongoClient(host, port)\n",
    "        self.collection = self.conn[db][collection]\n",
    "        self.field = field\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.collection.count_documents({})\n",
    "    \n",
    "    def iterate_from(self, start):\n",
    "        f = lambda d: d.get(self.field, '')\n",
    "        return iter(LazyMap(f, self.collection.find(projection={self.field: 1}, skip=start)))\n",
    "\n",
    "\n",
    "class MongoDBCorpusReader:\n",
    "    def __init__(self, word_tokenizer=TreebankWordTokenizer(),\n",
    "        sent_tokenizer=LazyLoader('tokenizers/punkt/PY3/english.pickle'),**kwargs):\n",
    "        self._seq = MongoDBLazySequence(**kwargs)\n",
    "        self._word_tokenize = word_tokenizer.tokenize\n",
    "        self._sent_tokenize = sent_tokenizer.tokenize\n",
    "\n",
    "    def text(self):\n",
    "        return self._seq\n",
    "    \n",
    "    def words(self):\n",
    "        return LazyConcatenation(LazyMap(self._word_tokenize, self.text()))\n",
    "    \n",
    "    def sents(self):\n",
    "        return LazyConcatenation(LazyMap(self._sent_tokenize, self.text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed_documents =  pd.read_csv(os.path.join(raw_data_path, \"publicru-dataset-2020-03-16\", \"lib_public_document.csv\"), index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_processed_documents[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_api_id</th>\n",
       "      <th>title</th>\n",
       "      <th>annotation</th>\n",
       "      <th>body</th>\n",
       "      <th>pages</th>\n",
       "      <th>authors</th>\n",
       "      <th>size</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>issue_id</th>\n",
       "      <th>pages_visual</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>182582609</td>\n",
       "      <td>Эффект слабого рубля</td>\n",
       "      <td>Мы взяли две компании с сопоставимой выручкой ...</td>\n",
       "      <td>&lt;body&gt;&lt;p&gt;&lt;b&gt;Мы взяли две компании с сопоставим...</td>\n",
       "      <td>[70, 71]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40754</td>\n",
       "      <td>2016-11-24 13:15:42.057148+00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>['70', '71']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21225</th>\n",
       "      <td>175907949</td>\n",
       "      <td>РОССТАТ</td>\n",
       "      <td>Индексы потребительских цен на товары и услуги...</td>\n",
       "      <td>&lt;body&gt;&lt;p&gt;&lt;b&gt;Индексы потребительских цен на тов...</td>\n",
       "      <td>[4]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28483</td>\n",
       "      <td>2016-11-24 23:46:59.007647+00:00</td>\n",
       "      <td>599</td>\n",
       "      <td>['4']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21226</th>\n",
       "      <td>175907948</td>\n",
       "      <td>Неотложка с планшетом</td>\n",
       "      <td>ЗДОРОВЬЕ . В системе ЕМИАС зарегистрировались ...</td>\n",
       "      <td>&lt;body&gt;&lt;p&gt;&lt;b&gt;ЗДОРОВЬЕ&lt;/b&gt; &lt;/p&gt;\\n&lt;p&gt;В системе ЕМ...</td>\n",
       "      <td>[4]</td>\n",
       "      <td>Сергей Жуков</td>\n",
       "      <td>30346</td>\n",
       "      <td>2016-11-24 23:46:59.090845+00:00</td>\n",
       "      <td>599</td>\n",
       "      <td>['4']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>183704356</td>\n",
       "      <td>ПРОГНОЗЫ НОМЕРА</td>\n",
       "      <td>К 2017-2018 годам ныне растущий агрокомплекс с...</td>\n",
       "      <td>&lt;body&gt;&lt;p&gt;К 2017-2018 годам ныне растущий агрок...</td>\n",
       "      <td>[5]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>818</td>\n",
       "      <td>2016-11-24 14:16:32.322323+00:00</td>\n",
       "      <td>42</td>\n",
       "      <td>['5']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>154615279</td>\n",
       "      <td>ТАНЕЦ НЕНАСТОЯЩЕГО ЛЕБЕДЯ</td>\n",
       "      <td>ПЕРЕД НАМИ КАК БУДТО СЦЕНА ИЗ БАЛЕТА: ДВА МИСТ...</td>\n",
       "      <td>&lt;body&gt;&lt;p&gt;ПЕРЕД НАМИ КАК БУДТО СЦЕНА ИЗ БАЛЕТА:...</td>\n",
       "      <td>[60, 61]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44550</td>\n",
       "      <td>2016-11-24 13:01:38.412930+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>['60', '61']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       content_api_id                      title  \\\n",
       "id                                                 \n",
       "325         182582609       Эффект слабого рубля   \n",
       "21225       175907949                    РОССТАТ   \n",
       "21226       175907948      Неотложка с планшетом   \n",
       "1638        183704356            ПРОГНОЗЫ НОМЕРА   \n",
       "2           154615279  ТАНЕЦ НЕНАСТОЯЩЕГО ЛЕБЕДЯ   \n",
       "\n",
       "                                              annotation  \\\n",
       "id                                                         \n",
       "325    Мы взяли две компании с сопоставимой выручкой ...   \n",
       "21225  Индексы потребительских цен на товары и услуги...   \n",
       "21226  ЗДОРОВЬЕ . В системе ЕМИАС зарегистрировались ...   \n",
       "1638   К 2017-2018 годам ныне растущий агрокомплекс с...   \n",
       "2      ПЕРЕД НАМИ КАК БУДТО СЦЕНА ИЗ БАЛЕТА: ДВА МИСТ...   \n",
       "\n",
       "                                                    body     pages  \\\n",
       "id                                                                   \n",
       "325    <body><p><b>Мы взяли две компании с сопоставим...  [70, 71]   \n",
       "21225  <body><p><b>Индексы потребительских цен на тов...       [4]   \n",
       "21226  <body><p><b>ЗДОРОВЬЕ</b> </p>\\n<p>В системе ЕМ...       [4]   \n",
       "1638   <body><p>К 2017-2018 годам ныне растущий агрок...       [5]   \n",
       "2      <body><p>ПЕРЕД НАМИ КАК БУДТО СЦЕНА ИЗ БАЛЕТА:...  [60, 61]   \n",
       "\n",
       "            authors   size                     last_modified  issue_id  \\\n",
       "id                                                                       \n",
       "325             NaN  40754  2016-11-24 13:15:42.057148+00:00         9   \n",
       "21225           NaN  28483  2016-11-24 23:46:59.007647+00:00       599   \n",
       "21226  Сергей Жуков  30346  2016-11-24 23:46:59.090845+00:00       599   \n",
       "1638            NaN    818  2016-11-24 14:16:32.322323+00:00        42   \n",
       "2               NaN  44550  2016-11-24 13:01:38.412930+00:00         1   \n",
       "\n",
       "       pages_visual  \n",
       "id                   \n",
       "325    ['70', '71']  \n",
       "21225         ['4']  \n",
       "21226         ['4']  \n",
       "1638          ['5']  \n",
       "2      ['60', '61']  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, soup=True):\n",
    "        self.load_stop_words()\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "        self.soup = soup\n",
    "        self.meaning_cache = {}  # Кеш значимых слов.\n",
    "        self.insignificant_cache = {}  # Кеш незначимых слов.\n",
    "        self.pos = ['NOUN', 'ADJF', 'ADJS', 'VERB', 'INFN', 'ADVB']\n",
    "        self.sentence_min_len = 5\n",
    "\n",
    "    def load_stop_words(self):\n",
    "        with open(os.path.join(external_data_path, 'russian_stop_words.txt'), 'r') as handle:\n",
    "            stop_words_ru = [line.rstrip('\\n') for line in handle]\n",
    "        with open(os.path.join(external_data_path, 'english_stop_words.txt'), 'r') as handle:\n",
    "            stop_words_en = [line.rstrip('\\n') for line in handle]\n",
    "        self.stop_words = set(stop_words_ru + stop_words_en)\n",
    "\n",
    "    # Фильтруем по части речи и возвращаем только начальную форму.\n",
    "    def lemmatize(self, tokens, need_pos=False):\n",
    "        words = []\n",
    "        for token in tokens:\n",
    "            # Если токен уже был закеширован, быстро возьмем результат из кэша.\n",
    "            if token in self.meaning_cache.keys():\n",
    "                words.append(self.meaning_cache[token])\n",
    "            elif token in self.insignificant_cache.keys():\n",
    "                pass\n",
    "            # Слово еще не встретилось, будем проводить медленный морфологический анализ.\n",
    "            else:\n",
    "                result = self.morph.parse(token)\n",
    "                if result[0].tag.POS != None:\n",
    "                    if result[0].tag.POS in self.pos:\n",
    "                        if need_pos:\n",
    "                            word = result[0].normal_form + \"_\" + result[0].tag.POS\n",
    "                        else:\n",
    "                            word = result[0].normal_form\n",
    "                        # Отправляем слово в результат, ...\n",
    "                        words.append(word)\n",
    "                        # ... и кешируем результат его разбора.\n",
    "                        self.meaning_cache[token] = word\n",
    "                    else:\n",
    "                        self.insignificant_cache[token] = \"\"\n",
    "                if 'LATN' in result[0].tag:\n",
    "                    if need_pos:\n",
    "                        word = result[0].normal_form + \"_\" + result[0].tag.POS\n",
    "                    else:\n",
    "                        word = result[0].normal_form\n",
    "                    words.append(word)\n",
    "                    self.meaning_cache[token] = word\n",
    "\n",
    "        return words\n",
    "\n",
    "    def normalize(self, document, exclude_short_sent):\n",
    "        if self.soup:\n",
    "            soup = BeautifulSoup(str(document), 'html.parser')\n",
    "            document = soup.get_text()\n",
    "        doc_tokens = []\n",
    "        for sentence in sent_tokenize(document):\n",
    "            sentence = sentence.replace(\"«\", \"\").replace(\"»\", \"\").replace(\"/\", \" \")\n",
    "            sentence = sentence.replace(\"№\", \"\").replace(\"-\", \" \").replace(\"–\", \" \").replace(\":\", \" \").replace(\"/\", \" \")\n",
    "            sentence = sentence.replace(\"ё\", \"е\")\n",
    "            tokens = word_tokenize(sentence)\n",
    "            if exclude_short_sent and len(tokens) < self.sentence_min_len:\n",
    "                tokens = []\n",
    "                continue\n",
    "            tokens = [i.lower() for i in tokens if i not in string.punctuation]\n",
    "            tokens = [i for i in tokens if i not in self.stop_words]\n",
    "            doc_tokens.extend(self.lemmatize(tokens, False))\n",
    "        return doc_tokens\n",
    "\n",
    "    def fit_transform(self, documents, exclude_short_sent=True):\n",
    "        result = []\n",
    "        for doc in tqdm(documents):\n",
    "            result.append(self.normalize(doc, exclude_short_sent))\n",
    "        return result\n",
    "\n",
    "    def save(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'wb') as handle:\n",
    "                pickle.dump([self.meaning_cache, self.insignificant_cache], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        except IOError:\n",
    "            logging.info(\"File not accessible\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'rb') as handle:\n",
    "                self.meaning_cache, self.insignificant_cache = pickle.load(handle)\n",
    "        except IOError:\n",
    "            logging.info(\"File not accessible\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing mystem to /home/science/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmas: красивый мама красиво мыть рама\n",
      "\n",
      "full info: [{\"analysis\": [{\"lex\": \"красивый\", \"wt\": 1, \"gr\": \"A=им,ед,полн,жен\"}], \"text\": \"Красивая\"}, {\"text\": \" \"}, {\"analysis\": [{\"lex\": \"мама\", \"wt\": 1, \"gr\": \"S,жен,од=им,ед\"}], \"text\": \"мама\"}, {\"text\": \" \"}, {\"analysis\": [{\"lex\": \"красиво\", \"wt\": 0.8149252476, \"gr\": \"ADV=\"}], \"text\": \"красиво\"}, {\"text\": \" \"}, {\"analysis\": [{\"lex\": \"мыть\", \"wt\": 0.441520999, \"gr\": \"V,несов,пе=прош,ед,изъяв,жен\"}], \"text\": \"мыла\"}, {\"text\": \" \"}, {\"analysis\": [{\"lex\": \"рама\", \"wt\": 0.9993591156, \"gr\": \"S,жен,неод=вин,ед\"}], \"text\": \"раму\"}, {\"text\": \"\\n\"}]\n"
     ]
    }
   ],
   "source": [
    "text = \"Красивая мама красиво мыла раму\"\n",
    "m = Mystem()\n",
    "lemmas = m.lemmatize(text)\n",
    "\n",
    "print(\"lemmas:\", ''.join(lemmas))\n",
    "print(\"full info:\", json.dumps(m.analyze(text), ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmas: у мы нет мыло\n",
      "\n",
      "full info: [{\"analysis\": [{\"lex\": \"у\", \"wt\": 0.9993940324, \"gr\": \"PR=\"}], \"text\": \"У\"}, {\"text\": \" \"}, {\"analysis\": [{\"lex\": \"мы\", \"wt\": 1, \"gr\": \"SPRO,мн,1-л=(пр|вин|род)\"}], \"text\": \"нас\"}, {\"text\": \" \"}, {\"analysis\": [{\"lex\": \"нет\", \"wt\": 0.464233437, \"gr\": \"ADV,прдк=\"}], \"text\": \"нет\"}, {\"text\": \" \"}, {\"analysis\": [{\"lex\": \"мыло\", \"wt\": 0.558479001, \"gr\": \"S,сред,неод=(вин,мн|род,ед|им,мн)\"}], \"text\": \"мыла\"}, {\"text\": \"\\n\"}]\n"
     ]
    }
   ],
   "source": [
    "text = \"У нас нет мыла\"\n",
    "lemmas = m.lemmatize(text)\n",
    "print(\"lemmas:\", ''.join(lemmas))\n",
    "print(\"full info:\", json.dumps(m.analyze(text), ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmas: You should never modify something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect.\n",
      "\n",
      "full info: [{\"analysis\": [], \"text\": \"You\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"should\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"never\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"modify\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"something\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"you\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"are\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"iterating\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"over\"}, {\"text\": \". \"}, {\"analysis\": [], \"text\": \"This\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"is\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"not\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"guaranteed\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"to\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"work\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"in\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"all\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"cases\"}, {\"text\": \". \"}, {\"analysis\": [], \"text\": \"Depending\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"on\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"the\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"data\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"types\"}, {\"text\": \", \"}, {\"analysis\": [], \"text\": \"the\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"iterator\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"returns\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"a\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"copy\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"and\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"not\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"a\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"view\"}, {\"text\": \", \"}, {\"analysis\": [], \"text\": \"and\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"writing\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"to\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"it\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"will\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"have\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"no\"}, {\"text\": \" \"}, {\"analysis\": [], \"text\": \"effect\"}, {\"text\": \".\"}, {\"text\": \"\\n\"}]\n"
     ]
    }
   ],
   "source": [
    "text = \"You should never modify something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect.\"\n",
    "\n",
    "lemmas = m.lemmatize(text)\n",
    "print(\"lemmas:\", ''.join(lemmas))\n",
    "print(\"full info:\", json.dumps(m.analyze(text), ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parse(word='у', tag=OpencorporaTag('PREP'), normal_form='у', score=0.995135, methods_stack=((<DictionaryAnalyzer>, 'у', 24, 0),)),\n",
      " Parse(word='у', tag=OpencorporaTag('INTJ'), normal_form='у', score=0.004864, methods_stack=((<DictionaryAnalyzer>, 'у', 21, 0),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,masc,Sgtm,Name,Fixd,Abbr,Init sing,nomn'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedFirstNameAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,masc,Sgtm,Name,Fixd,Abbr,Init sing,gent'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedFirstNameAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,masc,Sgtm,Name,Fixd,Abbr,Init sing,datv'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedFirstNameAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,masc,Sgtm,Name,Fixd,Abbr,Init sing,accs'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedFirstNameAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,masc,Sgtm,Name,Fixd,Abbr,Init sing,ablt'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedFirstNameAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,masc,Sgtm,Name,Fixd,Abbr,Init sing,loct'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedFirstNameAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,femn,Sgtm,Name,Fixd,Abbr,Init sing,nomn'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedFirstNameAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,femn,Sgtm,Name,Fixd,Abbr,Init sing,gent'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedFirstNameAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,femn,Sgtm,Name,Fixd,Abbr,Init sing,datv'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedFirstNameAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,femn,Sgtm,Name,Fixd,Abbr,Init sing,accs'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedFirstNameAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,femn,Sgtm,Name,Fixd,Abbr,Init sing,ablt'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedFirstNameAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,femn,Sgtm,Name,Fixd,Abbr,Init sing,loct'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedFirstNameAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,masc,Sgtm,Patr,Fixd,Abbr,Init sing,nomn'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedPatronymicAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,masc,Sgtm,Patr,Fixd,Abbr,Init sing,gent'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedPatronymicAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,masc,Sgtm,Patr,Fixd,Abbr,Init sing,datv'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedPatronymicAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,masc,Sgtm,Patr,Fixd,Abbr,Init sing,accs'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedPatronymicAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,masc,Sgtm,Patr,Fixd,Abbr,Init sing,ablt'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedPatronymicAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,masc,Sgtm,Patr,Fixd,Abbr,Init sing,loct'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedPatronymicAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,femn,Sgtm,Patr,Fixd,Abbr,Init sing,nomn'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedPatronymicAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,femn,Sgtm,Patr,Fixd,Abbr,Init sing,gent'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedPatronymicAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,femn,Sgtm,Patr,Fixd,Abbr,Init sing,datv'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedPatronymicAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,femn,Sgtm,Patr,Fixd,Abbr,Init sing,accs'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedPatronymicAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,femn,Sgtm,Patr,Fixd,Abbr,Init sing,ablt'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedPatronymicAnalyzer>, 'У'),)),\n",
      " Parse(word='у', tag=OpencorporaTag('NOUN,anim,femn,Sgtm,Patr,Fixd,Abbr,Init sing,loct'), normal_form='у', score=0.0, methods_stack=((<AbbreviatedPatronymicAnalyzer>, 'У'),))]\n",
      "[Parse(word='нас', tag=OpencorporaTag('NPRO,1per plur,gent'), normal_form='мы', score=0.648734, methods_stack=((<DictionaryAnalyzer>, 'нас', 1990, 1),)),\n",
      " Parse(word='нас', tag=OpencorporaTag('NPRO,1per plur,accs'), normal_form='мы', score=0.325949, methods_stack=((<DictionaryAnalyzer>, 'нас', 1990, 3),)),\n",
      " Parse(word='нас', tag=OpencorporaTag('NPRO,1per plur,loct'), normal_form='мы', score=0.025316, methods_stack=((<DictionaryAnalyzer>, 'нас', 1990, 5),))]\n",
      "[Parse(word='нет', tag=OpencorporaTag('PRED,pres'), normal_form='нет', score=0.571428, methods_stack=((<DictionaryAnalyzer>, 'нет', 242, 0),)),\n",
      " Parse(word='нет', tag=OpencorporaTag('PRCL'), normal_form='нет', score=0.412698, methods_stack=((<DictionaryAnalyzer>, 'нет', 22, 0),)),\n",
      " Parse(word='нет', tag=OpencorporaTag('INTJ'), normal_form='нет', score=0.015873, methods_stack=((<DictionaryAnalyzer>, 'нет', 21, 0),))]\n",
      "[Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut sing,gent'), normal_form='мыло', score=0.333333, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 1),)),\n",
      " Parse(word='мыла', tag=OpencorporaTag('VERB,impf,tran femn,sing,past,indc'), normal_form='мыть', score=0.333333, methods_stack=((<DictionaryAnalyzer>, 'мыла', 1813, 8),)),\n",
      " Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut plur,nomn'), normal_form='мыло', score=0.166666, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 6),)),\n",
      " Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut plur,accs'), normal_form='мыло', score=0.166666, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 9),))]\n"
     ]
    }
   ],
   "source": [
    "text = \"У нас нет мыла\"\n",
    "for sentence in sent_tokenize(text):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        pprint(morph.parse(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parse(word='красивая', tag=OpencorporaTag('ADJF,Qual femn,sing,nomn'), normal_form='красивый', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'красивая', 1800, 7),))]\n",
      "[Parse(word='мама', tag=OpencorporaTag('NOUN,anim,femn sing,nomn'), normal_form='мама', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'мама', 1907, 0),))]\n",
      "[Parse(word='красиво', tag=OpencorporaTag('ADVB'), normal_form='красиво', score=0.8, methods_stack=((<DictionaryAnalyzer>, 'красиво', 3, 0),)),\n",
      " Parse(word='красиво', tag=OpencorporaTag('ADJS,Qual neut,sing'), normal_form='красивый', score=0.2, methods_stack=((<DictionaryAnalyzer>, 'красиво', 1800, 56),))]\n",
      "[Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut sing,gent'), normal_form='мыло', score=0.333333, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 1),)),\n",
      " Parse(word='мыла', tag=OpencorporaTag('VERB,impf,tran femn,sing,past,indc'), normal_form='мыть', score=0.333333, methods_stack=((<DictionaryAnalyzer>, 'мыла', 1813, 8),)),\n",
      " Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut plur,nomn'), normal_form='мыло', score=0.166666, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 6),)),\n",
      " Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut plur,accs'), normal_form='мыло', score=0.166666, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 9),))]\n",
      "[Parse(word='раму', tag=OpencorporaTag('NOUN,inan,masc,Geox sing,datv'), normal_form='рам', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'раму', 32, 2),)),\n",
      " Parse(word='раму', tag=OpencorporaTag('NOUN,inan,femn sing,accs'), normal_form='рама', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'раму', 55, 3),))]\n"
     ]
    }
   ],
   "source": [
    "text = \"Красивая мама красиво мыла раму\"\n",
    "for sentence in sent_tokenize(text):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        pprint(morph.parse(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
