{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings  \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from typing import Callable, List, Tuple, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "import catalyst\n",
    "from catalyst.utils import imread\n",
    "from catalyst.dl import utils\n",
    "from catalyst.utils import get_dataset_labeling, map_dataframe\n",
    "\n",
    "from catalyst.utils import split_dataframe_train_test\n",
    "from catalyst.data.reader import ImageReader, ScalarReader, ReaderCompose\n",
    "from catalyst.data.augmentor import Augmentor\n",
    "from catalyst.dl.runner import SupervisedRunner\n",
    "from catalyst.dl.callbacks import AccuracyCallback, AUCCallback, F1ScoreCallback, CheckpointCallback, EarlyStoppingCallback\n",
    "\n",
    "from ignite.engine import Engine, _prepare_batch\n",
    "from ignite.engine import create_supervised_trainer\n",
    "from ignite.engine import create_supervised_evaluator\n",
    "from ignite.engine import Events\n",
    "from ignite.metrics import Loss, Accuracy, Precision, Recall\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.handlers import EarlyStopping\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict, OrderedDict\n",
    "from tqdm.notebook import tqdm\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"starter_code_01\"\n",
    "\n",
    "class ConfigExperiment:\n",
    "    logdir = f\"./logs/{EXPERIMENT_NAME}\"\n",
    "    save_dirname = EXPERIMENT_NAME\n",
    "    submission_file = f\"{EXPERIMENT_NAME}.csv\"\n",
    "    seed = 42\n",
    "    batch_size = 8\n",
    "    model_name = 'efficientnet-b0'\n",
    "    size = 512\n",
    "    num_workers = 20\n",
    "    root_images = \"../../../data/raw/plant-pathology-2020-fgvc7/images/\"\n",
    "    root = \"../../../data/raw/plant-pathology-2020-fgvc7/\"\n",
    "    num_classes = 4\n",
    "    patience= 5\n",
    "    num_epochs = 200\n",
    "    lr = 0.003\n",
    "    class_names = [\"healthy\", \"multiple_diseases\", \"rust\", \"scab\"]\n",
    "    is_fp16_used = False\n",
    "    log_interval = 50\n",
    "    \n",
    "config = ConfigExperiment()\n",
    "config.size = EfficientNet.get_image_size(config.model_name)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "utils.set_global_seed(config.seed)\n",
    "utils.prepare_cudnn(deterministic=True)\n",
    "device = utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, config, transforms=None):\n",
    "    \n",
    "        self.df = df\n",
    "        self.images_dir = config.root_images\n",
    "        self.class_names = config.class_names\n",
    "        self.transforms=transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_src = self.images_dir + self.df.iloc[idx]['image_id'] + '.jpg'\n",
    "        image = cv2.imread(image_src, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        labels = self.df.iloc[idx][self.class_names].values.astype(np.int8)\n",
    "        label = torch.argmax(torch.from_numpy(labels))\n",
    "        \n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=image)\n",
    "            image = transformed['image']\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_transforms(image_size=224):\n",
    "    # Convert the image to a square of size image_size x image_size\n",
    "    # (keeping aspect ratio)\n",
    "    result = [\n",
    "        A.LongestMaxSize(max_size=image_size),\n",
    "        A.PadIfNeeded(image_size, image_size, border_mode=0)\n",
    "    ]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def hard_transforms():\n",
    "    result = [\n",
    "        # Random shifts, stretches and turns with a 50% probability\n",
    "        A.RandomResizedCrop(height=config.size, width=config.size, p=1.0),\n",
    "        A.Flip(),\n",
    "        A.ShiftScaleRotate(rotate_limit=1.0, p=0.8),\n",
    "\n",
    "        # Pixels\n",
    "        A.OneOf([\n",
    "            A.IAAEmboss(p=1.0),\n",
    "            A.IAASharpen(p=1.0),\n",
    "            A.Blur(p=1.0),\n",
    "        ], p=0.5),\n",
    "\n",
    "        # Affine\n",
    "        A.OneOf([\n",
    "            A.ElasticTransform(p=1.0),\n",
    "            A.IAAPiecewiseAffine(p=1.0)\n",
    "        ], p=0.5),\n",
    "    ]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def post_transforms():\n",
    "    # we use ImageNet image normalization\n",
    "    # and convert it to torch.Tensor\n",
    "    return [A.Normalize(p=1.0), ToTensorV2(p=1.0),]\n",
    "\n",
    "def compose(transforms_to_compose):\n",
    "    # combine all augmentations into one single pipeline\n",
    "    result = A.Compose([item for sublist in transforms_to_compose for item in sublist])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(config.root + 'train.csv')\n",
    "train, valid = train_test_split(train_df, test_size=0.33, random_state=config.seed, shuffle=True, stratify=train_df[config.class_names])\n",
    "\n",
    "train_transforms = compose([\n",
    "    pre_transforms(config.size),\n",
    "#     hard_transforms(), \n",
    "    post_transforms()\n",
    "])\n",
    "valid_transforms = compose([\n",
    "    pre_transforms(config.size), \n",
    "    post_transforms()\n",
    "])\n",
    "\n",
    "show_transforms = compose([\n",
    "    pre_transforms(config.size),\n",
    "#     hard_transforms()\n",
    "])\n",
    "train_dataset = PlantDataset(train, config, train_transforms)\n",
    "valid_dataset = PlantDataset(valid, config, valid_transforms)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)\n",
    "\n",
    "\n",
    "indices = np.arange(len(train_dataset))\n",
    "random_indices = np.random.permutation(indices)[:len(valid_dataset)]\n",
    "train_subset = Subset(train_dataset, indices=random_indices)\n",
    "\n",
    "train_eval_loader = DataLoader(train_subset, batch_size=config.batch_size, shuffle=True,\n",
    "                                num_workers=config.num_workers, \n",
    "                                drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "def get_model(model_name: str, num_classes: int, pretrained: str = \"imagenet\") -> EfficientNet:\n",
    "    model = EfficientNet.from_pretrained(model_name)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = model._fc.in_features\n",
    "    model._fc = nn.Sequential(nn.Linear(num_ftrs, num_classes, bias = True))\n",
    "    return model\n",
    "\n",
    "model = get_model(config.model_name, config.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=config.patience, verbose=True, mode=\"min\", factor=0.3)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.RemovableEventHandle at 0x7fc538284208>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = {\n",
    "    'avg_loss': Loss(criterion),\n",
    "    'avg_accuracy': Accuracy(),\n",
    "    'avg_precision': Precision(average=True),\n",
    "    'avg_recall': Recall(average=True)\n",
    "}\n",
    "\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device)\n",
    "train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "val_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    iteration = (engine.state.iteration - 1) % len(train_dataloader) + 1\n",
    "    if iteration % config.log_interval == 0:\n",
    "        print(\"Epoch[{}] Iteration[{}/{}] Loss: {:.4f}\".format(engine.state.epoch, iteration, len(train_dataloader), engine.state.output))\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def compute_and_display_offline_train_metrics(engine):\n",
    "    epoch = engine.state.epoch\n",
    "    print(\"Compute train metrics...\")\n",
    "    metrics = train_evaluator.run(train_eval_loader).metrics\n",
    "    print(\"Training Results - Epoch: {}  Average Loss: {:.4f} | Accuracy: {:.4f} | Precision: {:.4f} | Recall: {:.4f}\"\n",
    "          .format(engine.state.epoch, \n",
    "                      metrics['avg_loss'], \n",
    "                      metrics['avg_accuracy'], \n",
    "                      metrics['avg_precision'], \n",
    "                      metrics['avg_recall']))\n",
    "\n",
    "    \n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def compute_and_display_val_metrics(engine):\n",
    "    epoch = engine.state.epoch\n",
    "    print(\"Compute validation metrics...\")\n",
    "    metrics = val_evaluator.run(valid_dataloader).metrics\n",
    "    print(\"Validation Results - Epoch: {}  Average Loss: {:.4f} | Accuracy: {:.4f} | Precision: {:.4f} | Recall: {:.4f}\"\n",
    "          .format(engine.state.epoch, \n",
    "                      metrics['avg_loss'], \n",
    "                      metrics['avg_accuracy'], \n",
    "                      metrics['avg_precision'], \n",
    "                      metrics['avg_recall']))\n",
    "\n",
    "    \n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def update_lr_scheduler(engine):\n",
    "    scheduler.step()\n",
    "    # Вывод значений скорости обучения:\n",
    "    if len(optimizer.param_groups) == 1:\n",
    "        lr = float(optimizer.param_groups[0]['lr'])\n",
    "        print(\"Learning rate: {}\".format(lr))\n",
    "    else:\n",
    "        for i, param_group in enumerate(optimizer.param_groups):\n",
    "            lr = float(param_group['lr'])\n",
    "            print(\"Learning rate (group {}): {}\".format(i, lr))\n",
    "            \n",
    "\n",
    "def score_function(engine):\n",
    "    val_avg_accuracy = engine.state.metrics['avg_accuracy']\n",
    "    return val_avg_accuracy\n",
    "\n",
    "\n",
    "best_model_saver = ModelCheckpoint(\n",
    "    f\"{config.save_dirname}_best\",  \n",
    "    filename_prefix=\"model\",\n",
    "    score_name=\"val_accuracy\",  \n",
    "    score_function=score_function,\n",
    "    n_saved=1,\n",
    "    require_empty=True,\n",
    "    save_as_state_dict=True,\n",
    "    create_dir=True\n",
    ")\n",
    "\n",
    "val_evaluator.add_event_handler(\n",
    "    Events.COMPLETED, \n",
    "    best_model_saver, \n",
    "    {\"best_model\": model}\n",
    ")\n",
    "\n",
    "\n",
    "training_saver = ModelCheckpoint(\n",
    "    f\"{config.save_dirname}_checkpoint\",\n",
    "    filename_prefix=\"checkpoint\",\n",
    "    n_saved=1,\n",
    "    save_as_state_dict=True,\n",
    "    create_dir=True\n",
    ")\n",
    "\n",
    "to_save = {\"model\": model, \"optimizer\": optimizer, \"scheduler\": scheduler} \n",
    "trainer.add_event_handler(Events.ITERATION_COMPLETED(every=100), training_saver, to_save)\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=config.patience,\n",
    "    score_function=score_function,\n",
    "    trainer=trainer\n",
    ")\n",
    "\n",
    "val_evaluator.add_event_handler(Events.EPOCH_COMPLETED, early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Iteration[50/153] Loss: 1.1698\n",
      "Epoch[1] Iteration[100/153] Loss: 0.6756\n",
      "Epoch[1] Iteration[150/153] Loss: 0.7308\n",
      "Compute train metrics...\n",
      "Training Results - Epoch: 1  Average Loss: 0.6444 | Accuracy: 0.7567 | Precision: 0.5840 | Recall: 0.6049\n",
      "Compute validation metrics...\n",
      "Validation Results - Epoch: 1  Average Loss: 0.6370 | Accuracy: 0.7687 | Precision: 0.5902 | Recall: 0.6106\n",
      "Learning rate: 0.0024000000000000002\n",
      "Epoch[2] Iteration[50/153] Loss: 0.7603\n",
      "Epoch[2] Iteration[100/153] Loss: 0.4699\n",
      "Epoch[2] Iteration[150/153] Loss: 0.3752\n",
      "Compute train metrics...\n",
      "Training Results - Epoch: 2  Average Loss: 0.4938 | Accuracy: 0.8050 | Precision: 0.8202 | Recall: 0.6762\n",
      "Compute validation metrics...\n",
      "Validation Results - Epoch: 2  Average Loss: 0.6003 | Accuracy: 0.7854 | Precision: 0.6796 | Recall: 0.6270\n",
      "Learning rate: 0.0019200000000000003\n",
      "Epoch[3] Iteration[50/153] Loss: 0.5728\n",
      "Epoch[3] Iteration[100/153] Loss: 0.7329\n",
      "Epoch[3] Iteration[150/153] Loss: 0.5498\n",
      "Compute train metrics...\n",
      "Training Results - Epoch: 3  Average Loss: 0.4159 | Accuracy: 0.8683 | Precision: 0.8565 | Recall: 0.7766\n",
      "Compute validation metrics...\n",
      "Validation Results - Epoch: 3  Average Loss: 0.5880 | Accuracy: 0.7987 | Precision: 0.6544 | Recall: 0.6404\n",
      "Learning rate: 0.0015360000000000003\n",
      "Epoch[4] Iteration[50/153] Loss: 0.8792\n",
      "Epoch[4] Iteration[100/153] Loss: 0.7884\n",
      "Epoch[4] Iteration[150/153] Loss: 1.2778\n",
      "Compute train metrics...\n",
      "Training Results - Epoch: 4  Average Loss: 0.4016 | Accuracy: 0.8567 | Precision: 0.8275 | Recall: 0.7368\n",
      "Compute validation metrics...\n",
      "Validation Results - Epoch: 4  Average Loss: 0.5775 | Accuracy: 0.7854 | Precision: 0.6318 | Recall: 0.6283\n",
      "Learning rate: 0.0012288000000000004\n",
      "Epoch[5] Iteration[50/153] Loss: 0.5987\n",
      "Epoch[5] Iteration[100/153] Loss: 0.4316\n",
      "Epoch[5] Iteration[150/153] Loss: 0.5930\n",
      "Compute train metrics...\n",
      "Training Results - Epoch: 5  Average Loss: 0.3787 | Accuracy: 0.8800 | Precision: 0.8450 | Recall: 0.8275\n",
      "Compute validation metrics...\n",
      "Validation Results - Epoch: 5  Average Loss: 0.5706 | Accuracy: 0.8053 | Precision: 0.7002 | Recall: 0.6847\n",
      "Learning rate: 0.0009830400000000003\n",
      "Epoch[6] Iteration[50/153] Loss: 0.6532\n",
      "Epoch[6] Iteration[100/153] Loss: 0.5818\n",
      "Epoch[6] Iteration[150/153] Loss: 1.1880\n",
      "Compute train metrics...\n",
      "Training Results - Epoch: 6  Average Loss: 0.3559 | Accuracy: 0.8800 | Precision: 0.9074 | Recall: 0.7758\n",
      "Compute validation metrics...\n",
      "Validation Results - Epoch: 6  Average Loss: 0.5646 | Accuracy: 0.8070 | Precision: 0.6422 | Recall: 0.6427\n",
      "Learning rate: 0.0007864320000000003\n",
      "Epoch[7] Iteration[50/153] Loss: 0.7060\n",
      "Epoch[7] Iteration[100/153] Loss: 0.7054\n",
      "Epoch[7] Iteration[150/153] Loss: 0.7672\n",
      "Compute train metrics...\n",
      "Training Results - Epoch: 7  Average Loss: 0.3636 | Accuracy: 0.8717 | Precision: 0.8700 | Recall: 0.7788\n",
      "Compute validation metrics...\n",
      "Validation Results - Epoch: 7  Average Loss: 0.5746 | Accuracy: 0.7953 | Precision: 0.6838 | Recall: 0.6506\n",
      "Learning rate: 0.0006291456000000003\n",
      "Epoch[8] Iteration[50/153] Loss: 0.5186\n",
      "Epoch[8] Iteration[100/153] Loss: 0.2742\n",
      "Epoch[8] Iteration[150/153] Loss: 0.4066\n",
      "Compute train metrics...\n",
      "Training Results - Epoch: 8  Average Loss: 0.3357 | Accuracy: 0.8983 | Precision: 0.9073 | Recall: 0.8272\n",
      "Compute validation metrics...\n",
      "Validation Results - Epoch: 8  Average Loss: 0.5661 | Accuracy: 0.8020 | Precision: 0.6768 | Recall: 0.6538\n",
      "Learning rate: 0.0005033164800000003\n",
      "Epoch[9] Iteration[50/153] Loss: 0.2242\n",
      "Epoch[9] Iteration[100/153] Loss: 0.6225\n",
      "Epoch[9] Iteration[150/153] Loss: 0.6666\n",
      "Compute train metrics...\n",
      "Training Results - Epoch: 9  Average Loss: 0.3280 | Accuracy: 0.9017 | Precision: 0.9092 | Recall: 0.8211\n",
      "Compute validation metrics...\n",
      "Validation Results - Epoch: 9  Average Loss: 0.5720 | Accuracy: 0.7970 | Precision: 0.6575 | Recall: 0.6404\n",
      "Learning rate: 0.0004026531840000003\n",
      "Epoch[10] Iteration[50/153] Loss: 0.3135\n",
      "Epoch[10] Iteration[100/153] Loss: 1.3920\n",
      "Epoch[10] Iteration[150/153] Loss: 0.6127\n",
      "Compute train metrics...\n",
      "Training Results - Epoch: 10  Average Loss: 0.3289 | Accuracy: 0.8933 | Precision: 0.8920 | Recall: 0.8238\n",
      "Compute validation metrics...\n",
      "Validation Results - Epoch: 10  Average Loss: 0.5685 | Accuracy: 0.8053 | Precision: 0.7101 | Recall: 0.6639\n",
      "Learning rate: 0.00032212254720000025\n",
      "Epoch[11] Iteration[50/153] Loss: 0.4558\n",
      "Epoch[11] Iteration[100/153] Loss: 0.2662\n",
      "Epoch[11] Iteration[150/153] Loss: 1.5103\n",
      "Compute train metrics...\n",
      "Training Results - Epoch: 11  Average Loss: 0.3345 | Accuracy: 0.8750 | Precision: 0.8772 | Recall: 0.7951\n",
      "Compute validation metrics...\n",
      "Validation Results - Epoch: 11  Average Loss: 0.5727 | Accuracy: 0.7970 | Precision: 0.7080 | Recall: 0.6587\n",
      "Learning rate: 0.0002576980377600002\n"
     ]
    }
   ],
   "source": [
    "output = trainer.run(train_dataloader, max_epochs=config.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from ignite._utils import convert_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-535cd328c645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconvert_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"indices\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0minferencer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEngine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minference_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def _prepare_batch(batch):\n",
    "    x, index = batch\n",
    "    x = convert_tensor(x, device=device)\n",
    "    return x, index\n",
    "\n",
    "def inference_update(engine, batch):\n",
    "    x, indices = _prepare_batch(batch)\n",
    "    y_pred = model(x)\n",
    "    y_pred = F.softmax(y_pred, dim=1)\n",
    "    return {\"y_pred\": convert_tensor(y_pred, device='cpu'), \"indices\": indices}\n",
    "\n",
    "model.eval()\n",
    "inferencer = Engine(inference_update)\n",
    "\n",
    "@inferencer.on(Events.EPOCH_COMPLETED)\n",
    "def log_tta(engine):\n",
    "    print(\"TTA {} / {}\".format(engine.state.epoch, n_tta))\n",
    "    \n",
    "n_tta = 5\n",
    "num_classes = 4\n",
    "n_samples = len(valid_dataset)\n",
    "\n",
    "# Массив для хранения предсказаний\n",
    "y_probas_tta = np.zeros((n_samples, num_classes, n_tta), dtype=np.float32)\n",
    "\n",
    "\n",
    "@inferencer.on(Events.ITERATION_COMPLETED)\n",
    "def save_results(engine):\n",
    "    output = engine.state.output\n",
    "    tta_index = engine.state.epoch - 1\n",
    "    start_index = ((engine.state.iteration - 1) % len(valid_dataloader)) * batch_size\n",
    "    end_index = min(start_index + batch_size, n_samples)\n",
    "    batch_y_probas = output['y_pred'].detach().numpy()\n",
    "    y_probas_tta[start_index:end_index, :, tta_index] = batch_y_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id\n",
       "0   Test_0\n",
       "1   Test_1\n",
       "2   Test_2\n",
       "3   Test_3\n",
       "4   Test_4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = create_dataset(root_dir=config.root_images, mask=\"Test*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path_test = create_dataframe(dataset_test, columns=[\"image_id\", \"filepath\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path_test[\"healthy\"] = 0\n",
    "df_path_test[\"multiple_diseases\"] = 0\n",
    "df_path_test[\"rust\"] = 0\n",
    "df_path_test[\"scab\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>filepath</th>\n",
       "      <th>healthy</th>\n",
       "      <th>multiple_diseases</th>\n",
       "      <th>rust</th>\n",
       "      <th>scab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0</td>\n",
       "      <td>../../../data/raw/plant-pathology-2020-fgvc7/i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_1</td>\n",
       "      <td>../../../data/raw/plant-pathology-2020-fgvc7/i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_10</td>\n",
       "      <td>../../../data/raw/plant-pathology-2020-fgvc7/i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_100</td>\n",
       "      <td>../../../data/raw/plant-pathology-2020-fgvc7/i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_1000</td>\n",
       "      <td>../../../data/raw/plant-pathology-2020-fgvc7/i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id                                           filepath  healthy  \\\n",
       "0     Test_0  ../../../data/raw/plant-pathology-2020-fgvc7/i...        0   \n",
       "1     Test_1  ../../../data/raw/plant-pathology-2020-fgvc7/i...        0   \n",
       "2    Test_10  ../../../data/raw/plant-pathology-2020-fgvc7/i...        0   \n",
       "3   Test_100  ../../../data/raw/plant-pathology-2020-fgvc7/i...        0   \n",
       "4  Test_1000  ../../../data/raw/plant-pathology-2020-fgvc7/i...        0   \n",
       "\n",
       "   multiple_diseases  rust  scab  \n",
       "0                  0     0     0  \n",
       "1                  0     0     0  \n",
       "2                  0     0     0  \n",
       "3                  0     0     0  \n",
       "4                  0     0     0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_path_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict(model: torch.nn.Module, config: ConfigExperiment, class_names: List[str], images_df: pd.DataFrame, device: torch.device) -> pd.DataFrame:\n",
    "    result = []\n",
    "    for _, row in images_df.iterrows():\n",
    "        path = row[\"filepath\"]\n",
    "        _image = imread(path)\n",
    "    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tensor_ = torch.stack([valid_transforms(image=_image)[\"image\"]]).to(device)\n",
    "            logits = model.forward(tensor_)\n",
    "            probabilities = softmax(logits, dim=1)\n",
    "            predictions = probabilities.argmax(dim=1)\n",
    "            result.append(probabilities.cpu().numpy()[0])\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 8s, sys: 1.5 s, total: 6min 10s\n",
      "Wall time: 4min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_path_test[['healthy', 'multiple_diseases', 'rust', 'scab']] = make_predict(model, config, class_names=config.class_names, images_df=df_path_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>filepath</th>\n",
       "      <th>healthy</th>\n",
       "      <th>multiple_diseases</th>\n",
       "      <th>rust</th>\n",
       "      <th>scab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0</td>\n",
       "      <td>../../../data/raw/plant-pathology-2020-fgvc7/i...</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.998304</td>\n",
       "      <td>5.175470e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_1</td>\n",
       "      <td>../../../data/raw/plant-pathology-2020-fgvc7/i...</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.011641</td>\n",
       "      <td>0.988051</td>\n",
       "      <td>1.286567e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_10</td>\n",
       "      <td>../../../data/raw/plant-pathology-2020-fgvc7/i...</td>\n",
       "      <td>0.011240</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.988737</td>\n",
       "      <td>2.179752e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_100</td>\n",
       "      <td>../../../data/raw/plant-pathology-2020-fgvc7/i...</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.029997</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>9.698012e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_1000</td>\n",
       "      <td>../../../data/raw/plant-pathology-2020-fgvc7/i...</td>\n",
       "      <td>0.684439</td>\n",
       "      <td>0.024830</td>\n",
       "      <td>0.267419</td>\n",
       "      <td>2.331294e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id                                           filepath   healthy  \\\n",
       "0     Test_0  ../../../data/raw/plant-pathology-2020-fgvc7/i...  0.001694   \n",
       "1     Test_1  ../../../data/raw/plant-pathology-2020-fgvc7/i...  0.000180   \n",
       "2    Test_10  ../../../data/raw/plant-pathology-2020-fgvc7/i...  0.011240   \n",
       "3   Test_100  ../../../data/raw/plant-pathology-2020-fgvc7/i...  0.000166   \n",
       "4  Test_1000  ../../../data/raw/plant-pathology-2020-fgvc7/i...  0.684439   \n",
       "\n",
       "   multiple_diseases      rust          scab  \n",
       "0           0.000001  0.998304  5.175470e-09  \n",
       "1           0.011641  0.988051  1.286567e-04  \n",
       "2           0.000023  0.988737  2.179752e-07  \n",
       "3           0.029997  0.000036  9.698012e-01  \n",
       "4           0.024830  0.267419  2.331294e-02  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_path_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path_test.drop([\"filepath\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path_test.to_csv(config.submission_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>healthy</th>\n",
       "      <th>multiple_diseases</th>\n",
       "      <th>rust</th>\n",
       "      <th>scab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.998304</td>\n",
       "      <td>5.175470e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_1</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.011641</td>\n",
       "      <td>0.988051</td>\n",
       "      <td>1.286567e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_10</td>\n",
       "      <td>0.011240</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.988737</td>\n",
       "      <td>2.179752e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_100</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.029997</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>9.698012e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_1000</td>\n",
       "      <td>0.684439</td>\n",
       "      <td>0.024830</td>\n",
       "      <td>0.267419</td>\n",
       "      <td>2.331294e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id   healthy  multiple_diseases      rust          scab\n",
       "0     Test_0  0.001694           0.000001  0.998304  5.175470e-09\n",
       "1     Test_1  0.000180           0.011641  0.988051  1.286567e-04\n",
       "2    Test_10  0.011240           0.000023  0.988737  2.179752e-07\n",
       "3   Test_100  0.000166           0.029997  0.000036  9.698012e-01\n",
       "4  Test_1000  0.684439           0.024830  0.267419  2.331294e-02"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_path_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
